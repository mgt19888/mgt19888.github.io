<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>《Easy RL:强化学习教程》读书笔记01</title>
    <url>/post/61f51f4.html</url>
    <content><![CDATA[<h1>强化学习概述</h1>
<p>强化学习由<ins>智能体（agent）<ins>和</ins>环境（environment）<ins>组成，智能体得到环境中的</ins>状态（state）<ins>后，将根据状态输出一个</ins>动作（action）</ins>，这个动作也是<ins>决策（decision）</ins>，这个动作会在环境中被执行后输出状态和<ins>奖励（reward</ins>），智能体的目标是最大化所获得的奖励。</p>
<h2 id="强化学习和监督学习的区别">强化学习和监督学习的区别</h2>
<p>1、强化学习输入的样本是序列数据，监督学习的样本的独立同分布的。</p>
<p>2、强化学习只能通过不停地尝试去做出最有利的动作，而监督学习是有人类知识引导的。</p>
<p>3、<ins>探索（exploration）<ins>和</ins>利用（exploitation）<ins>是强化学习里面非常核心的问题，<ins>探索</ins>是尝试新动作以获得更高的奖励，但有一定的风险，而</ins>利用</ins>是采取已知的可获最大奖励的动作，所以需要在<ins>探索</ins>和<ins>利用</ins>间进行权衡，这是监督学习中没有的。</p>
<p>4、强化学习中的奖励一般是延迟的，而监督学习是即时的监督。</p>
<h2 id="标准强化学习和深度强化学习的区别">标准强化学习和深度强化学习的区别</h2>
<p><ins>标准强化学习</ins>需要设计特征、进行特征工程并设计价值函数，而<ins>深度强化学习</ins>是一个端到端的训练过程，可直接通过神经网络来拟合价值函数或策略网络。</p>
<p><img src="https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/image-20220816210507041.png" alt="image-20220816210507041"></p>
<h2 id="状态和观测的区别">状态和观测的区别</h2>
<p><ins>状态</ins>是对世界的完整描述，不会隐藏世界的信息。<ins>观测</ins>是对状态的部分描述，可能会遗漏一些信</p>
<p>息。如果状态和观察等价，则该环境是完全可观测的（fully observed），这种情况也被称为<ins>马尔可夫决策过程（Markov decision process，MDP）</ins>。当智能体只能看到部分的观测时，我们就称这个环境是<ins>部分可观测的（partially observed）</ins>，这种情况也被称为<ins>部分可观测马尔可夫决策过程（partially observable Markov decision process, POMDP）</ins>。部分可观测马尔可夫决</p>
<p>策过程可以用一个七元组描述：(<em>S, A, T, R, Ω, O, γ</em>)。其中 <em>S</em> 表示状态空间，为隐变量，<em>A</em> 为动作空间，<em>T</em>(s′|s, a) 为状态转移概率，<em>R</em> 为奖励函数，<em>Ω</em>(<em>o++|++s, a</em>) 为观测概率，<em>O</em> 为观测空间，<em>γ</em> 为折扣因子。</p>
<h1>强化学习智能体</h1>
<h2 id="组成成分">组成成分</h2>
<p>对于一个强化学习智能体，也是<ins>马尔可夫决策过程（Markov decision process）</ins>，它可能有一个或多个如下的组成成分。</p>
<h3 id="策略（policy）">策略（policy）</h3>
<p>智能体会用策略来选取下一步的动作。策略可分为随机性策略和确定性策略。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>++随机性策略（stochastic policy）++也是概率函数，即输入一个状态后输出一个概率，智能体根据该动作概率采取动作。</p>
</li>
<li class="lvl-2">
<p>++确定性策略（deterministic policy）++是智能体直接采取最有可能的动作。</p>
</li>
</ul>
<p>通常情况下，强化学习一般使用随机性策略，随机性策略有很多优点，其具有多样性，能够更好地探索环境。</p>
<h3 id="价值函数（value-function）">价值函数（value function）</h3>
<p>用于评估智能体进入某个状态后，可以对后面的奖励带来多大的影响。价值函数值越大，说明智能体进入这个状态越有利。价值函数里面有一个<ins>折扣因子（discount factor）</ins>，折扣因子越大，越关注未来对现在的影响。常用的价值函数是Q函数，Q 函数里面包含状态和动作两个变量，即未来可以获得奖励的期望取决于当前的状态和当前的动作。</p>
<h3 id="模型（model）">模型（model）</h3>
<p>模型表示智能体对环境的状态进行理解，它决定了环境中世界的运行方式。它由状态转移概率和奖励函数两个部分组成</p>
<h2 id="强化学习智能体的类型">强化学习智能体的类型</h2>
<ul class="lvl-0">
<li class="lvl-2">
<p>++基于价值的智能体（value-based agent）++显式地学习价值函数，隐式地学习它的策略。策略是其从学到的价值函数里面推算出来的。</p>
</li>
<li class="lvl-2">
<p>++基于策略的智能体（policy-based agent）++直接学习策略，我们给它一个状态，它就会输出对应动作的概率。基于策略的智能体并没有学习价值函数。</p>
</li>
<li class="lvl-2">
<p>++演员-评论员智能体（actor-critic agent）++把策略和价值函数都学习了，然后通过两者的交互得到最佳动作。</p>
</li>
</ul>
<h3 id="基于策略和基于价值的强化学习方法有什么区别">基于策略和基于价值的强化学习方法有什么区别?</h3>
<p>在基于策略的强化学习方法中，智能体会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。而在基于价值的强化学习方法中，智能体不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。基于价值迭代的方法只能应用在不连续的、离散的环境下（如围棋或某些游戏领域），对于动作集合规模庞大、动作连续的场景（如机器人控制领域），其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作）。基于价值的强化学习算法有 Q 学习（Q-learning）、Sarsa等，而基于策略的强化学习算法有策略梯度（Policy Gradient，PG）算法等。</p>
<h3 id="有模型强化学习和免模型强化学习有什么区别？">有模型强化学习和免模型强化学习有什么区别？</h3>
<p>有模型强化学习是指根据环境中的经验，构建一个虚拟世界，同时在真实环境和虚拟世界中学习；免模型强化学习是指不对环境进行建模，直接与真实环境进行交互来学习到最优策略。</p>
<p>免模型强化学习通常属于数据驱动型方法，需要大量的采样来估计状态、动作及奖励函数，从而优化动作策略。相比之下，有模型的深度强化学习可以在一定程度上缓解训练数据匮乏的问题，因为智能体可以在虚拟世界中进行训练。免模型学习的泛化性要优于有模型强化学习，原因是有模型强化学习算需要对真实环境进行建模，并且虚拟世界与真实环境之间可能还有差异，这限制了有模型强化学习算法的泛化性。</p>
<p>目前，大部分深度强化学习方法都采用了免模型强化学习，这是因为：免模型强化学习更为简单、直观且有丰富的开源资料</p>
<blockquote>
<p>第一章习题链接：<a href="https://datawhalechina.github.io/easy-rl/#/chapter1/chapter1_questions&amp;keywords">https://datawhalechina.github.io/easy-rl/#/chapter1/chapter1_questions&amp;keywords</a></p>
</blockquote>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>《Easy RL:强化学习教程》读书笔记03</title>
    <url>/post/e81130d8.html</url>
    <content><![CDATA[<h1>策略梯度算法</h1>
<p>在强化学习中，我们唯一需要做的就是调整演员里面的策略，使得演员可以得到最大的奖励。演员里面的策略决定了演员的动作，即给定一个输入，它会输出演员现在应该要执行的动作。策略一般记作π。假设我们使用深度学习来做强化学习，策略就是一个网络。网络里面有一些参数，我们用 θ来代表π的参数。网络的输入是智能体看到的东西，如果让智能体玩视频游戏，智能体看到的东西就是游戏的画面。智能体看到的东西会影响我们训练的效果。例如，在玩游戏的时候， 也许我们觉得游戏的画面是前后相关的，所以应该让策略去看从游戏开始到当前这个时间点之间所有画面的总和。因此我们可能会觉得要用到循环神经网络（recurrent neural network，RNN）来处理它，不过这样会比较难处理。我们可以用向量或矩阵来表示智能体的观测，并将观测输入策略网络，策略网络就会输出智能体要采取的动作。因为我们要让奖励越大越好，所以可以使用++梯度上升（gradient ascent）<ins>来最大化期望奖励。一般</ins>策略梯度（policy gradient，PG）++采样的数据只会用一次。我们采样这些数据，然后用这些数据更新参数，再丢掉这些数据。接着重新采样数据，才能去更新参数。</p>
<h1>策略梯度实现技巧</h1>
<h2 id="技巧-1：添加基线">技巧 1：添加基线</h2>
<p>第一个技巧：添加基线（baseline）。如果给定状态 s采取动作 a，整场游戏得到正的奖励，就要增加 (s,a)的概率。如果给定状态 s执行动作 a，整场游戏得到负的奖励，就要减小 (s,a)的概率。但在很多游戏里面，奖励总是正的，最低都是 0。比如打乒乓球游戏， 分数为 0 ~ 21 分，所以R(τ)总是正的。</p>
<p>为了解决奖励总是正的的问题，我们可以把奖励减 b，其中，b称为基线。通过这种方法，我们就可以让 R(τ)−b 这一项有正有负。如果我们得到的总奖励R(τ)&gt;b，就让(s,a) 的概率上升。如果R(τ)&lt;b，就算R(τ)是正的，值很小也是不好的，我们就让(s,a)的概率下降，让这个状态采取这个动作的分数下降。b怎么设置呢？我们可以对 τ的值取期望， 计算τ的平均值，令b≈E[R(τ)]。 所以在训练的时候，我们会不断地把 R(τ) 的值记录下来，会不断地计算R(τ) 的平均值，把这个平均值当作 b来使用。 这样就可以让我们在训练的时候，R(τ)−b 是有正有负的，这是第一个技巧。</p>
<h2 id="技巧-2：分配合适的分数">技巧 2：分配合适的分数</h2>
<p>给每一个动作分配合适的分数（credit）。如果在同一个回合里面，在同一场游戏里面，所有的状态-动作对使用同样的奖励项进行加权，那这样显然是不公平的，因为在同一场游戏里面，也许有些动作是好的，有些动作是不好的。</p>
<p>一个做法是计算某个状态-动作对的奖励的时候，不把整场游戏得到的奖励全部加起来，只计算从这个动作执行以后得到的奖励。因为这场游戏在执行这个动作之前发生的事情是与执行这个动作是没有关系的，所以在执行这个动作之前得到的奖励都不能算是这个动作的贡献。我们把执行这个动作以后发生的所有奖励加起来，才是这个动作真正的贡献。</p>
<p>接下来更进一步，我们把未来的奖励做一个折扣，为什么要把未来的奖励做一个折扣呢？因为虽然在某一时刻，执行某一个动作，会影响接下来所有的结果（有可能在某一时刻执行的动作，接下来得到的奖励都是这个动作的功劳），但在一般的情况下，时间拖得越长，该动作的影响力就越小。 比如在第2个时刻执行某一个动作， 那在第3个时刻得到的奖励可能是在第2个时刻执行某个动作的功劳，但是在第 100 个时刻之后又得到奖励，那可能就不是在第2个时刻执行某一个动作的功劳。</p>
<h2 id="综合以上技巧">综合以上技巧</h2>
<p>实际上就是这么实现的。b可以是依赖状态（state-dependent）的，事实上 b通常是一个网络估计出来的，它是一个网络的输出。我们把 R-b这一项称为<ins>优势函数（advantage function）</ins>。优势函数取决于 s和 a，我们就是要计算在某个状态 s 采取某个动作 a 的时候，优势函数的值。优势函数的意义是，假设我们在某一个状态s_t 执行某一个动作 a_t，相较于其他可能的动作，a_t有多好。优势函数在意的不是绝对的好，而是相对的好，即<ins>相对优势（relative advantage）</ins>。因为在优势函数中，我们会减去一个基线 b，所以这个动作是相对的好，不是绝对的好。 优势函数通常可以由一个网络估计出来，这个网络称为评论员（critic）。</p>
<h1>on-policy 和 off-policy 的区别</h1>
<ul class="lvl-0">
<li class="lvl-2">
<p>如果要学习的 agent 跟和环境互动的 agent 是同一个的话， 这个叫做<ins>on-policy(同策略)</ins>。</p>
</li>
<li class="lvl-2">
<p>如果要学习的 agent 跟和环境互动的 agent 不是同一个的话， 那这个叫做<ins>off-policy(异策略)</ins>。</p>
</li>
</ul>
<h1>重要性采样(Importance Sampling，IS)</h1>
<p>对于ー个随机变量，通常用概率密度函数来刻画该变量的概率分布特性。具体来说，给定随机变量的一个取值，可以根据概率密度函数来计算该值对应的概率（密度）。反过来，也可以根据概率密度函数提供的概率分布信息来生成随机变量的一个取值，这就是采样。因此，从某种意义上来说，采样是概率密度函数的逆向应用。与根据概率密度函数计算样本点对应的概率值不同，采样过程往往没有那么直接，通常需要根据待采样分布的具体特点来选择合适的采样策略。</p>
<p><ins>重要性采样有一些问题。<ins>虽然理论上你可以把 p 换成任何的 q。但是在实现上，p 和 q 不能差太多。差太多的话，会有一些问题。所以需要使用</ins>Proximal Policy Optimization (PPO)<ins>来约束p和q所输出的动作的</ins>KL 散度(KL divergence)</ins></p>
<blockquote>
<p>第四章习题链接：<a href="https://datawhalechina.github.io/easy-rl/#/chapter4/chapter4_questions&amp;keywords">https://datawhalechina.github.io/easy-rl/#/chapter4/chapter4_questions&amp;keywords</a></p>
<p>第五章习题链接：<a href="https://datawhalechina.github.io/easy-rl/#/chapter5/chapter5_questions&amp;keywords">https://datawhalechina.github.io/easy-rl/#/chapter5/chapter5_questions&amp;keywords</a></p>
</blockquote>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>《Easy RL:强化学习教程》读书笔记02</title>
    <url>/post/9f16004e.html</url>
    <content><![CDATA[<h1>策略迭代</h1>
<p>策略迭代由两个步骤组成：策略评估和策略改进（policy improvement）。第一个步骤是策略评估，当前我们在优化策略 \pi<em>π</em>，在优化过程中得到一个最新的策略。我们先保证这个策略不变，然后估计它的价值，即给定当前的策略函数来估计状态价值函数。 第二个步骤是策略改进，得到 状态价值函数后，我们可以进一步推算出它的 Q 函数。得到 Q 函数后，我们直接对 Q 函数进行最大化，通过在 Q 函数做一个贪心的搜索来进一步改进策略。这两个步骤一直在迭代进行。</p>
<h1>价值迭代</h1>
<p>价值迭代做的工作类似于价值的反向传播，每次迭代做一步传播，所以中间过程的策略和价值函数 是没有意义的。而策略迭代的每一次迭代的结果都是有意义的，都是一个完整的策略。如图所示为一个可视化的求最短路径的过程，在一个网格世界中，我们设定了一个终点，也就是左上角的点。不管我们在哪一个位置开始，我们都希望能够到达终点（实际上这个终点在迭代过程中是不必要的，只是为了更好地演示）。价值迭代的迭代过程像是一个从某一个状态（这里是我们的终点）反向传播到其他各个状态的过程，因为每次迭代只能影响到与之直接相关的状态。</p>
<p><img src="https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/2.52.png" alt="2.52"></p>
<h1>策略迭代和价值迭代的区别</h1>
<p>我们再来对比策略迭代和价值迭代，这两个算法都可以解马尔可夫决策过程的控制问题。策略迭代分两步。首先进行策略评估，即对当前已经搜索到的策略函数进行估值。得到估值后，我们进行策略改进，即把 Q 函数算出来，进行进一步改进。不断重复这两步，直到策略收敛。价值迭代直接使用贝尔曼最优方程进行迭代，从而寻找最佳的价值函数。找到最佳价值函数后，我们再提取最佳策略。</p>
<h1>马尔可夫决策过程中的预测和控制总结</h1>
<p>我们使用动态规划算法来解马尔可夫决策过程里面的预测和控制，并且采取不同的贝尔曼方程。对于预测问题，即策略评估的问题，我们不停地执行贝尔曼期望方程，这样就可以估计出给定的策略，然后得到价值函数。对于控制问题，如果我们采取的算法是策略迭代，使用的就是贝尔曼期望方程；如果我们采取的算法是价值迭代，使用的就是贝尔曼最优方程。</p>
<p><img src="https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/image-20220821172111216.png" alt="image-20220821172111216"></p>
<h1>动态规划方法和蒙特卡洛方法的差异</h1>
<p>蒙特卡洛方法通过一个回合的经验平均回报（实际得到的奖励）来进行更新。蒙特卡洛方法相比动态规划方法是有一些优势的。首先，蒙特卡洛方法适用于环境未知的情况，而动态规划是有模型的方法。蒙特卡洛方法只需要更新一条轨迹的状态，而动态规划方法需要更新所有的状态。状态数量很多的时候（比如 100 万个、200 万个），我们使用动态规划方法进行迭代，速度是非常慢的。这也是基于采样的蒙特卡洛方法相对于动态规划方法的优势。</p>
<h1>时序差分方法</h1>
<p>时序差分是介于蒙特卡洛和动态规划之间的方法，它是免模型的，不需要马尔可夫决策过程的转移矩阵和奖励函数。此外，时序差分方法可以从不完整的回合中学习，并且结合了自举的思想。</p>
<h1>时序差分方法和蒙特卡洛方法的差异</h1>
<p>（1）时序差分方法可以在线学习（online learning），每走一步就可以更新，效率高。蒙特卡洛方法必须等游戏结束时才可以学习。</p>
<p>（2）时序差分方法可以从不完整序列上进行学习。蒙特卡洛方法只能从完整的序列上进行学习。</p>
<p>（3）时序差分方法可以在连续的环境下（没有终止）进行学习。蒙特卡洛方法只能在有终止的情况下学习。</p>
<p>（4）时序差分方法利用了马尔可夫性质，在马尔可夫环境下有更高的学习效率。蒙特卡洛方法没有假设环境具有马尔可夫性质，利用采样的价值来估计某个状态的价值，在不是马尔可夫的环境下更加有效。</p>
<p>例子：时序差分方法是指在不清楚马尔可夫状态转移概率的情况下，以采样的方式得到不完整的状态序列，估计某状态在该状态序列完整后可能得到的奖励，并通过不断地采样持续更新价值。蒙特卡洛则需要经历完整的状态序列后，再来更新状态的真实价值。例如，我们想获得开车去公司的时间，每天上班开车的经历就是一次采样。假设我们今天在路口 A 遇到了堵车，时序差分方法会在路口 A 就开始更新预计到达路口 B、路口 C <em>· · · · · ·</em> ，以及到达公司的时间；而蒙特卡洛方法并不会立即更新时间，而是在到达公司后，再更新到达每个路口和公司的时间。时序差分方法能够在知道结果之前就开始学习，相比蒙特卡洛方法，其更快速、灵活。</p>
<h1>动态规划方法、蒙特卡洛方法以及时序差分方法的自举和采样</h1>
<p>自举是指更新时使用了估计。蒙特卡洛方法没有使用自举，因为它根据实际的回报进行更新。动态规划方法和时序差分方法使用了自举。</p>
<p>采样是指更新时通过采样得到一个期望。蒙特卡洛方法是纯采样的方法。动态规划方法没有使用采样，它是直接用贝尔曼期望方程来更新状态价值的。时序差分方法使用了采样。时序差分目标由两部分组成，一部分是采样，一部分是自举。</p>
<p><img src="https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/image-20220821213946200.png" alt="image-20220821213946200"></p>
<h1>Q学习和Sarsa的区别</h1>
<ul class="lvl-0">
<li class="lvl-2">
<p>Sarsa 是一个典型的同策略算法，它只用了一个策略 π，它不仅使用策略 π 学习，还使用策略 π与环境交互产生经验。 如果策略采用 ε-贪心算法，它需要兼顾探索，为了兼顾探索和利用，它训练的时候会显得有点“胆小”。它在解决悬崖行走问题的时候，会尽可能地远离悬崖边，确保哪怕自己不小心探索了一点儿，也还是在安全区域内。此外，因为采用的是 ε-贪心 算法，策略会不断改变（ε值会不断变小），所以策略不稳定。</p>
</li>
<li class="lvl-2">
<p>Q学习是一个典型的异策略算法，它有两种策略————目标策略和行为策略，它分离了目标策略与行为策略。Q学习可以大胆地用行为策略探索得到的经验轨迹来优化目标策略，从而更有可能探索到最佳策略。行为策略可以采用 ε-贪心 算法，但目标策略采用的是贪心算法，它直接根据行为策略采集到的数据来采用最佳策略，所以 Q学习 不需要兼顾探索。</p>
</li>
<li class="lvl-2">
<p>我们比较一下 Q学习 和 Sarsa 的更新公式，就可以发现 Sarsa 并没有选取最大值的最大化操作。因此，Q学习是一个非常激进的方法，它希望每一步都获得最大的利益；Sarsa 则相对较为保守，它会选择一条相对安全的迭代路线。</p>
</li>
</ul>
<blockquote>
<p>第二章习题链接：<a href="https://datawhalechina.github.io/easy-rl/#/chapter2/chapter2_questions&amp;keywords">https://datawhalechina.github.io/easy-rl/#/chapter2/chapter2_questions&amp;keywords</a></p>
<p>第三章习题链接：<a href="https://datawhalechina.github.io/easy-rl/#/chapter3/chapter3_questions&amp;keywords">https://datawhalechina.github.io/easy-rl/#/chapter3/chapter3_questions&amp;keywords</a></p>
</blockquote>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>《Easy RL:强化学习教程》读书笔记04</title>
    <url>/post/7675a57b.html</url>
    <content><![CDATA[<h1>DQN基本概念</h1>
<p>Deep Q-Network是基于深度学习的Q-learning算法，其结合了 Value Function Approximation（价值函数近似）与神经网络技术，并采用了目标网络（Target Network）和经验回放（Experience Replay）的方法进行网络的训练。</p>
<h2 id="状态价值函数">状态价值函数</h2>
<p>在基于价值的算法中，学到的不是策略，而是critic，即进行策略评估，衡量状态价值函数有两种不同的方法：基于蒙特卡洛的方法和基于时序差分的方法。基于<ins>蒙特卡洛</ins>的方法就是通过让actor与environment做交互，来让critic评价，其评价主要根据累积奖励值来的。第二种是<ins>时序差分</ins>的方法，即基于时序差分的方法。在基于蒙特卡洛的方法中，每次我们都要计算累积奖励，也就是从某一个状态一直到游戏结束的时候，得到的所有奖励的总和。但这样很受限制，得游戏结束才能更新网络，而<ins>时序差分</ins>是让前后两个状态价值函数值V(s_t)和V(s_t+1)的损失和r_t接近。</p>
<h2 id="动作价值函数">动作价值函数</h2>
<p>这种critic称为Q函数，其输入的是一个state-action pair，输出是累积奖励的期望值。我们可以估计某一个策略的Q函数，接下来就可以找到另外一个策略π′ 比原来的策略π还要更好。</p>
<p><img src="https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/%E5%8A%A8%E4%BD%9C%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0.png" alt="动作价值函数"></p>
<h1>DQN的技巧</h1>
<h2 id="目标网络">目标网络</h2>
<p>在实现的时候，我们会把左边的 Q 网络更新多次，再用更新过的 Q 网络替换目标网络。但这两个网络不要一起更新，一起更新，结果会很容易不好。一开始这两个网络是一样的，在训练的时候，我们会把右边的 Q 网络固定住，在做梯度下降的时候，只调整左边Q网络的参数。我们可能更新 100 次以后才把参数复制到右边的网络中，把右边网络的参数覆盖，目标值就变了。就好像我们本来在做一个回归问题，训练后把这个回归问题的损失降下去以后，接下来我们把左边网络的参数复制到右边网络，这样训练更加稳定。</p>
<h2 id="探索">探索</h2>
<p>探索是指agent对新action的探索过程。有两个方法可以解决<ins>探索-利用窘境（exploration-exploitation dilemma）<ins>问题：<ins>ε-贪心</ins>和</ins>玻尔兹曼探索（Boltzmann exploration）</ins>。</p>
<p><ins>ε-贪心</ins>是指我们有 1−ε的概率会按照Q函数来决定动作，ε 的概率进行随机决定动作。</p>
<p><ins>玻尔兹曼探索</ins>就比较像是 policy gradient。在 policy gradient 里面network 的output 是一个 expected action space 上面的一个的 probability distribution。再根据 probability distribution 去做 sample。所以也可以根据Q value 去定一个 probability distribution，假设某一个 action 的 Q value 越大，代表它越好，我们采取这个 action 的机率就越高。这是<ins>Boltzmann Exploration</ins>。</p>
<h2 id="经验回放">经验回放</h2>
<p>经验回放会构建一个<ins>回放缓冲区（replay buffer）</ins>，回放缓冲区又被称为<ins>回放内存（replay memory）</ins>。回放缓冲区是指现在有某一个策略π与环境交互，它会去收集数据，我们把所有的数据放到一个数据缓冲区（buffer）里面，数据缓冲区里面存储了很多数据。回放缓冲区只有在它装满的时候，才会把旧的数据丢掉。所以回放缓冲区里面其实装了很多不同的策略的经验。</p>
<p>有了回放缓冲区以后，我们怎么训练 Q 模型、怎么估 Q 函数呢？我们会迭代地训练 Q 函数，在每次迭代里面，从回放缓冲区中随机挑一个批量（batch）出来，即与一般的网络训练一样，从训练集里面挑一个批量出来。我们采样该批量出来，里面有一些经验，我们根据这些经验去更新Q函数。这与时序差分学习要有一个目标网络是一样的。我们采样一个批量的数据，得到一些经验，再去更新 Q 函数。</p>
<p>这么做有两个好处。第一个好处是，在进行强化学习的时候， 往往最花时间的步骤是与环境交互，训练网络反而是比较快的。因为我们用 GPU 训练其实很快，真正花时间的往往是与环境交互。用回放缓冲区可以减少与环境交互的次数，因为在做训练的时候，经验不需要通通来自于某一个策略。一些过去的策略所得到的经验可以放在回放缓冲区里面被使用很多次，被反复的再利用，这样可以比较高效地采样经验。第二个好处是，在训练网络的时候，其实我们希望一个批量里面的数据越多样（diverse）越好。如果批量里面的数据都是同样性质的，我们训练下去，训练结果是容易不好的。如果批量里面都是一样的数据，训练的时候，性能会比较差。我们希望批量里的数据越多样越好。如果回放缓冲区里面的经验通通来自于不同的策略，我们采样到的一个批量里面的数据会是比较多样的。</p>
<h2 id="双深度-Q-网络（Double-DQN）"><ins>双深度</ins> <ins>Q</ins> 网络（Double DQN）</h2>
<p>在实现上，Q 值往往是被高估的。而Double DQN 可以让估测的值跟实际的值是比较接近的。DDQN 里面有两个 Q 网络，第一个 Q 网络 Q 决定哪一个动作的 Q 值最大（我们把所有的 <em>a</em> 代 入 Q 函数中，看看哪一个 a 的 Q 值最大）。我们决定动作以后，Q 值是用 Q′ 算出来的。假设我们有两个 Q 函数——Q和 Q′，如果Q 高估了它选出来的动作 a，只要 Q′没有高估动作 <em>a</em> 的值，算出来的就还是正常的值。假设 Q′高估了某一个动作的值，也是没问题的，因为只要Q不选这个动作就可以，这就是 DDQN 神奇的地方。</p>
<h2 id="竞争深度-Q-网络（Dueling-DQN）"><ins>竞争深度</ins> <ins>Q</ins> 网络（Dueling DQN）</h2>
<p>其实 Dueling DQN 也蛮好做的，相较于原来的 DQN，它唯一的差别是改了网络的架构。Q-network 就是输入状态，输出就是每一个动作的 Q 值。Dueling DQN 唯一做的事情是改了网络的架构，其它的算法都不需要动。</p>
<p>原来的深度 Q 网络直接输出 Q 值，竞争深度 Q 网络不直接输出 Q 值，而是分成两条路径运算。第一条路径会输出一个标量V (s)，因为它与输入s是有关系的，所以称为 V(s)。第二条路径会输出一个向量 A(s,a)，它的每一个动作都有一个值。我们再把 V (s) 和 A(s,a) 加起来就可以得到 Q 值Q(s, a)。</p>
<h2 id="优先级经验回放（Prioritized-Experience-Replay）">优先级经验回放（Prioritized Experience Replay）</h2>
<p><img src="https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/image-20220826164817751.png" alt="image-20220826164817751"></p>
<h2 id="在蒙特卡洛方法和时序差分方法中取得平衡（Balance-between-MC-and-TD）">在蒙特卡洛方法和时序差分方法中取得平衡（Balance between MC and TD）</h2>
<p>该方法也被称为多步方法，多步方法就是蒙特卡洛方法与时序差分方法的结合，因此它不仅有蒙特卡洛方法的好处与坏处，还有时序差分方法的好处与坏处。我们先看看多步方法的好处，之前只采样了某一个步骤，所以得到的数据是真实的，接下来都是 Q 值估测出来的。现在采样比较多的步骤，采样 <em>N</em> 个步骤才估测值，所以估测的部分所造成的影响就会比较小。当然多步方法的坏处就与蒙特卡洛方法的坏处一样，因为 <em>r</em> 有比较多项，所以我们把 <em>N</em> 项的 <em>r</em> 加起来，方差就会比较大。但是我们可以调整 <em>N</em> 的值，在方差与不精确的 Q 值之间取得一个平衡。<em>N</em> 就是一个超参数，我们可以对其进行调整。</p>
<h2 id="噪声网络（Noisy-Net）">噪声网络（Noisy Net）</h2>
<p>我们还可以改进探索。ε-贪心这样的探索就是在动作的空间上加噪声，但是有一个更好的方法称为噪声网络（noisy net），它是在参数的空间上加噪声。噪声网络是指，每一次在一个回合开始的时候，在智能体要与环境交互的时候，智能体使用 Q 函数来采取动作，Q 函数里面就是一个网络，我们在网络的每一个参数上加上一个高斯噪声（Gaussian noise）。噪声在一个回合中是不能被改变的。</p>
<h2 id="分布式-Q-函数（Distributional-Q-function）">分布式 Q 函数（Distributional Q-function）</h2>
<p>我们可以不只是估测得到的期望 reward 平均值的值。我们其实是可以估测一个分布的。</p>
<h2 id="彩虹（Rainbow）">彩虹（Rainbow）</h2>
<p>最后一个技巧叫做 rainbow，把刚才所有的方法都综合起来就变成 rainbow 。因为刚才每一个方法，就是有一种自己的颜色，把所有的颜色通通都合起来，就变成 rainbow。</p>
<h1>演员-评论员算法</h1>
<p><ins>演员-评论员</ins>算法是一种结合<ins>策略梯度</ins>和<ins>时序差分学习</ins>的强化学习方法，其中，演员是指策略函数 πθ(a|s)，即学习一个策略以得到尽可能高的回报。评论员是指价值函数 Vπ(s)，对当前策略的值函数进行 估计，即评估演员的好坏。借助于价值函数，演员-评论员算法可以进行单步参数更新，不需要等到回合结束才进行更新。在演员-评论员算法里面，最知名的算法就是异步优势演员-评论员算法。如果我们去掉异步，则为<ins>优势演员-评论员（advantage actor-critic，A2C）算法</ins>。A2C算法又被译作优势演员-评论员算法。如果我们加了异步，变成异步优势演员-评论员算法。</p>
<h2 id="优势演员-评论员算法">优势演员-评论员算法</h2>
<p>优势演员-评论员算法的流程如图所示，我们有一个 π，有个初始的演员与环境交互，先收集资料。在策略梯度方法里收集资料以后，就来更新策略。但是在演员-评论员算法里面，我们不是直接使用那些资料来更新策略。我们先用这些资料去估计价值函数，可以用 时序差分方法 或 蒙特卡洛方法 来估计价值函数。接下来，我们再基于价值函数，有了新的 π 以后，再与环境交互，收集新的资料，去估计价值函数。再用新的价值函数更新策略，更新演员。整个优势演员-评论员算法就是这么运作的。</p>
<p><img src="https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/%E4%BC%98%E5%8A%BF%E8%AF%84%E8%AE%BA%E5%91%98-%E8%AF%84%E8%AE%BA%E5%91%98%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B.png" alt="优势评论员-评论员算法流程"></p>
<h2 id="异步优势演员-评论员算法">异步优势演员-评论员算法</h2>
<p>异步优势演员-评论员算法同时使用很多个进程（worker），如果我们没有很多 CPU，不好实现异步优势演员-评论员算法，但可以实现优势演员-评论员算法。</p>
<p>异步优势演员-评论员算法的运作流程如图所示，异步优势演员-评论员算法一开始有一个全局网络（global network）。全局网络包含策略网络和价值网络，这两个网络是绑定（tie）在一起的，它们的前几个层会被绑在一起。 我们使用多个进程，每个进程用一张 CPU 去跑。比如我们有 8 个进程，则至少 8 张 CPU。每一个进程在工作前都会把全局网络的参数复制过来。接下来演员就与环境交互，每一个演员与环境交互的时候，都要收集到比较多样的数据。例如，如果是走迷宫，可能每一个演员起始的位置都会不一样，这样它们才能够收集到比较多样的数据。每一个演员与环境交互完之后，我们就会计算出梯度。计算出梯度以后，要用梯度去更新参数。我们就计算一下梯度，用梯度去更新全局网络的参数。就是这个进程算出梯度以后，就把梯度传回给中央的控制中心，中央的控制中心就会用这个梯度去更新原来的参数。</p>
<p>注意，A3C使用了平行探索的方法，所有的演员都是平行跑的，每一个演员各做各的，不管彼此。所以每个演员都是去要了一个参数以后，做完就把参数传回去。</p>
<p><img src="https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/%E5%BC%82%E6%AD%A5%E4%BC%98%E5%8A%BF%E6%BC%94%E5%91%98-%E8%AF%84%E8%AE%BA%E5%91%98%E7%AE%97%E6%B3%95%E7%9A%84%E8%BF%90%E4%BD%9C%E6%B5%81%E7%A8%8B.png" alt="异步优势演员-评论员算法的运作流程"></p>
<h2 id="路径衍生策略梯度">路径衍生策略梯度</h2>
<p>一开始会有一个策略 π，它与环境交互并估计 Q 值。估计完 Q 值以后，我们就把 Q 值固定，只去学习一个演员。假设这个 Q 值估得很准，它知道在某一个状态采取什么样的动作会得到很大的Q值。接下来就学习这个演员，演员在给定 s 的时候，采取了a，可以让最后Q函数算出来的值越大越好。我们用准则（criteria）去更新策略 π，用新的 π 与环境交互，再估计 Q值，得到新的 π 去最大化 Q值的输出。深度Q网络 里面的技巧，在这里也几乎都用得上，比如经验回放、探索等技巧。</p>
<p><img src="https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/%E8%B7%AF%E5%BE%84%E8%A1%8D%E7%94%9F%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95.png" alt="路径衍生策略梯度算法"></p>
<blockquote>
<p>第六章习题：<a href="https://datawhalechina.github.io/easy-rl/#/chapter6/chapter6_questions&amp;keywords">https://datawhalechina.github.io/easy-rl/#/chapter6/chapter6_questions&amp;keywords</a></p>
<p>第七章习题：<a href="https://datawhalechina.github.io/easy-rl/#/chapter7/chapter7_questions&amp;keywords">https://datawhalechina.github.io/easy-rl/#/chapter7/chapter7_questions&amp;keywords</a></p>
<p>第八章习题：<a href="https://datawhalechina.github.io/easy-rl/#/chapter8/chapter8_questions&amp;keywords">https://datawhalechina.github.io/easy-rl/#/chapter8/chapter8_questions&amp;keywords</a></p>
<p>第九章习题：<a href="https://datawhalechina.github.io/easy-rl/#/chapter9/chapter9_questions&amp;keywords">https://datawhalechina.github.io/easy-rl/#/chapter9/chapter9_questions&amp;keywords</a></p>
</blockquote>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>《Easy RL:强化学习教程》读书笔记05</title>
    <url>/post/17295ed.html</url>
    <content><![CDATA[<h1>DDPG的改进</h1>
<p>虽然 DDPG 有时表现很好，但它对于超参数和其他类型的调整方面经常很敏感。DDPG 常见的问题是已经学习好的 Q 函数开始显著地高估 Q 值，然后导致策略被破坏，因为它利用了 Q 函数中的误差  。我们可以使用实际的 Q 值与 Q 网络输出的 Q 值进行对比。实际的 Q 值可以用蒙特卡洛来算。根据当前的策略采样 1000 条轨迹，得到 G 后取平均值，进而得到实际的 Q 值。</p>
<p>双延迟深度确定性策略梯度（twin delayed DDPG， TD3） 通过引入 3 个关键技巧来解决这个问题。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>截断的双 Q 学习（clipped dobule Q-learning）。 TD3 学习两个 Q 函数（因此名字中有“twin”）。TD3 通过最小化均方差来同时学习两个 Q 函数。两个 Q 函数都使用一个目标。</p>
</li>
<li class="lvl-2">
<p>延迟的策略更新（delayed policy updates） 。相关实验结果表明，同步训练动作网络和评价网络，却不使用目标网络，会导致训练过程不稳定；但是仅固定动作网络时，评价网络往往能够收敛到正确的结果。因此 TD3 算法以较低的频率更新动作网络，以较高的频率更新评价网络，通常每更新两次评价网络就更新一次策略。</p>
</li>
<li class="lvl-2">
<p>目标策略平滑（target policy smoothing）。 TD3 引入了平滑化（smoothing）思想。 TD3 在目标动作中加入噪声，通过平滑 Q 沿动作的变化，使策略更难利用 Q 函数的误差。</p>
</li>
</ul>
<p>这 3 个技巧加在一起，使得性能相比基线 DDPG 有了大幅的提升。</p>
<blockquote>
<p>第十章习题：<a href="https://datawhalechina.github.io/easy-rl/#/chapter10/chapter10_questions&amp;keywords">https://datawhalechina.github.io/easy-rl/#/chapter10/chapter10_questions&amp;keywords</a></p>
<p>第十一章习题：<a href="https://datawhalechina.github.io/easy-rl/#/chapter11/chapter11_questions&amp;keywords">https://datawhalechina.github.io/easy-rl/#/chapter11/chapter11_questions&amp;keywords</a></p>
<p>第十二章习题：<a href="https://datawhalechina.github.io/easy-rl/#/chapter12/chapter12_questions&amp;keywords">https://datawhalechina.github.io/easy-rl/#/chapter12/chapter12_questions&amp;keywords</a></p>
</blockquote>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>云原生初探</title>
    <url>/post/f3e182a8.html</url>
    <content><![CDATA[

	<div class="row">
    <embed src="/img/mydocument.pdf" width="100%" height="550" type="application/pdf">
	</div>



]]></content>
      <tags>
        <tag>云原生</tag>
      </tags>
  </entry>
  <entry>
    <title>大学本科经验分享</title>
    <url>/post/3610a686.html</url>
    <content><![CDATA[<h1>个人档案</h1>
<div align=center>
    <img src=https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/大学本科经验分享_1.png width=70%></img>
</div>
<h1>打卡笔记六 “不” 走</h1>
<h2 id="六边形战士-人生“不”设限">六边形战士,人生“不”设限</h2>
<p>​	大学以来，我除了做好专业学习和科研竞赛外，还<ins>积极参与到社会实践和学生工作中</ins>。身处大学这样一个充满机遇的平台，我们不能让自己仅仅局限在学习能力和科研能力的提高上。参与社会实践、学生工作能够<ins>认识到比自己优秀的同学，与优秀的同学为伍，自己也会变得更优秀。</ins></p>
<div align=center>
    <img src=https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/大学本科经验分享_2.png width=80%></img>
</div>
<h2 id="生活“不”abandon-英语学习很重要">生活“不”abandon,英语学习很重要</h2>
<p>​	在高校夏令营或预推免中，大部分双一流高校都要求通过六级，有些高校还会对六级成绩进行规定。如果没有过六级，很多高校的简历初筛都过不了。在保研面试中，也会有英文问答、英文自我介绍环节，这一部分的分数通常占比15%-40%。你的六级成绩越高分，在保研面试中就有越大的优势。</p>
<h2 id="勇者“不”退缩-不服就比比">勇者“不”退缩,不服就比比</h2>
<p>​	我一直都鼓励我的师弟师妹们++多参加比赛，因为比赛的过程中，我们会对专业知识有更深刻的理解，从而更快掌握专业知识。++对于工科生来说，可参加的比赛是相当多的，数学建模、数学竞赛、程序设计大赛、计算机设计大赛、互联网+、挑战杯等都值得参加。</p>
<p>​	在大一，我跟很多同学一样，会因为害怕当“分母”而不敢参赛。后来，我在老师和师兄师姐的鼓励下，大胆迈出第一步。刚开始参赛时，我基本拿不到奖项，但我依然不放弃，继续在参赛路上砥砺前行。不过比起结果，我<ins>更享受参赛过程中自己点滴的进步</ins>。因为参加了比较多的比赛，我学到了很多新知识，能力也有所提高，拥有了夯实的学科基础。</p>
<p>​	而在今年的互联网+大赛中，申报的两个赛道都取得了全国总决赛银奖的成绩，正所谓 “故不积跬步，无以至千里；不积小流，无以成江海。” 因此，我觉得参加创新创业比赛对我大学生涯的影响可以有以下几点：</p>
<p>​	<ins>（1）个人能力的提升。</ins> 如果仅仅是听课，创新创业思维是很难锻炼的，只有去真正的实践，在实践中遇到问题并解决问题，才能慢慢形成创新创业思维。而创新创业比赛便是最好的实践平台，在比赛中能够促进自己去自学新知识，开阔视野，将所学的专业知识运用于实践。如果作品足够优秀，还能够得到主办方一定的支持。也正是因为参加这些比赛，我的自学能力、动手实践能力、组织能力、沟通能力等等都得到一定的提高。</p>
<p>​	<ins>（2）认识到更多优秀的人。</ins> 创新创业比赛基本都是团体性的比赛，需要与其他同学合作，在这期间也能够认识到更多志同道合、优秀的同学，与优秀的同学为伍，自己也会变得更优秀。</p>
<p>​	<ins>（3）助力深造和就业。</ins> 通过参加创新创业比赛，在得到锻炼的同时，也能收获相应的奖项和荣誉，这些奖项能够证明自己的能力，在考研、保研或者是就业面试中都能够起到一些作用。</p>
<h2 id="学习步履“不”停-一“研”为定">学习步履“不”停,一“研”为定</h2>
<p>​	在本科阶段真正地参与一些科研项目，可以帮你<ins>认清实际的科研是怎样的，能够让你认识到自己是否适合读研</ins>。同时，科研项目也能够将所学的知识最大程度地串起来，帮助自己<ins>找到感兴趣的研究方向，从而进行更深入的研究学习</ins>。除此之外，科研项目也能够丰富自己的简历，让你在就业或考研、保研面试有更多的优势。</p>
<div align=center>
    <img src=https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/大学本科经验分享_3.png width=80%></img>
</div>
<h2 id="“不”拖延-打卡少年有妙招">“不”拖延,打卡少年有妙招</h2>
<p>​	学习和竞赛本身是不冲突的事情，学习专业知识能够在参赛时有更多理论支撑，参加竞赛能够让自己学以致用，加深对知识的理解与掌握。</p>
<p>​	关于时间管理，首先我是将时间分为三大部分，分别是寒暑假、每个学期的前半个学期、每个学期的后半个学期，寒暑假我主要参与科研项目、参加社会实践活动，每个学期的前半个学期主要是参与学生工作、参加比赛、做好学期规划，每个学期的后半个学期主要是学习专业知识、打牢专业基础。当然并不是每一个阶段只做这些事情，而是这些事情在相应阶段是主要的安排。我认为在每个阶段有侧重点去学习或工作，才能达到较高的效率。除此之外，每天晚上睡前我都会把第二天做的事情在手机备忘录上列出来，我不会做详细的计划（像哪个时间段做哪些事情），详细的计划会让我觉得很难执行，所以都是只列出要做的事情而不规定时间段，每完成一件事情就在备忘录左边打勾，这样也比较有成就感，能够让自己每天都过得很充实，并且很少有不知道做什么或者发呆的时候。</p>
<div align=center>
    <img src=https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/大学本科经验分享_4.png width=40%></img>
</div>
<h2 id="“不”忘初心-走向星辰大海">“不”忘初心,走向星辰大海</h2>
<p>​	保研是我大一就树立的目标，在过去的三年里，我一直在为这个目标而奋斗。虽然过程中遇到很多挫折，也曾让我怀疑过自己，但在身边朋友、老师和家人的帮助下，在不断尝试下，最终达成目标。</p>
<p>​	我建议在读大学的你们尽早确定方向，朝着自己的目标扬帆起航。当然，保研对绩点和科研竞赛有较高的要求。如果你选择了保研，就代表你将走上一条竞争激烈的道路。在走向成功的路上，不要因为小挫折而失去信心，要勇于抓住眼前的每一个机会。同时，也要善于总结与反思自己每个阶段所做的事情，并不断为自己树立小目标。</p>
<div align=center>
热血燃烧 青春闪耀</br>
未来在即 梦想可期</br>
愿大家都能实现自己的梦想！</br>
</div>]]></content>
      <tags>
        <tag>经验分享</tag>
      </tags>
  </entry>
  <entry>
    <title>对领域的认知比会写代码更重要</title>
    <url>/post/fcaac7db.html</url>
    <content><![CDATA[<h1>对领域的认知比会写代码更重要</h1>
<p>昨天看了一篇文章，觉得对我非常有启发，所以想做个转发分享出来。</p>
<h2 id="很少有人从头开始构建代码">很少有人从头开始构建代码</h2>
<p>在大学，他们会教你如何编写一个400行的程序，从头到尾地解决一个问题。你手头有一个白板，你需要展示一些花哨的算法，以证明你具有可以找到走出迷宫的知识和能力。最后，你找到了完美解决方案去解决那个超简单的问题。这看起来很像真实的世界，对吧？但现实并非如此。在现实世界中，你有一个几十万行的代码，当你试图弄清楚你的同事在写这部分代码时到底在抽什么风，你需要在文档和更了解代码的人之间来回切换。当一周快结束时，你写了10行代码来修复某个bug，然后循环往复，直到人们找到你，向你解释他当时为什么要这样写。</p>
<p>专业的软件开发人员在团队中工作，一般只处理大型软件代码库的一小部分，并且通常只是修复一些东西，而不是从头开始构建。它并不像培训机构所描述的那样迷人，而且修复所付出的开销要比写代码多得多。</p>
<h2 id="对领域的认知比会写代码更重要">对领域的认知比会写代码更重要</h2>
<p>我惊讶地发现，当你理解了它是如何（更重要的是为什么）工作，以及工作的基本原理后，编写代码会变得容易得多。</p>
<p>在构建银行移动端APP的时候，你可以更好地理解交易是如何运行的，货币结算是如何运作的，账本是如何工作的等。</p>
<p>在为餐厅构建销售系统时，你最好弄清楚服务员是如何工作的，在烹饪清单中如何做库存管理，以及信用卡授权是如何工作的。基本上，这就是你的软件运行域的内部和外部。</p>
<p>构建医疗、物流和记账方面的软件也是如此。</p>
<p>如果不了解这些，个体是很难做出有意义的贡献的，对雇主来说也就没有价值了。例如，如果你有银行APP的经验，那么你很容易在金融领域再找到一份新的工作，因为你已经熟悉该领域。</p>
<h2 id="文档编写没有得到足够重视">文档编写没有得到足够重视</h2>
<p>大学经常为学生提供软件开发职业所需的基本技术技能，如算法和数据结构。然而，他们通常不会优先考虑编写整洁、文档良好和可维护的代码。</p>
<p>通常，开发人员只有在处理别人编写的代码并经历了试图理解和修改它的挑战之后，才会意识到编写可维护代码的价值。可想而知，当我看到正确的文档时，我是多么高兴。然而，这些都不是在课堂上学到的，而是通过实践经验总结的。通过编写文档和易于理解的代码可以节省大量时间和精力。</p>
<h2 id="代码是次要的，商业价值才是第一的">代码是次要的，商业价值才是第一的</h2>
<p>没有人会过来对你说：“哇，这行代码写的真棒！”相反，他们会说：“用户对你写的功能相当满意”，或者“你的代码把整个网站都搞垮了”。</p>
<p>虽然这听起来可能令人惊讶，但软件工程师的主要工作重点不是编写代码，而是通过使用已编写的软件来创造价值。代码只是实现这一目标的工具。代码-&gt;软件-&gt;价值。</p>
<p>你写的东西需要满足世界上的一些需求——一些用户使用的工具，一些降低成本的自动化，一些人们愿意付出（付出他们的时间、金钱或注意力）的东西。我们可以简化它。如果你用糟糕的技术构建了一些为用户提供巨大价值的东西——恭喜你，你已经完成了作为软件工程师的目标。但，如果你用优秀的技术构建了一些东西，但却为用户提供了糟糕的价值——那么很遗憾，你并没有达到目标。</p>
<p>优雅的代码，最佳实践，智能解决方案，设计模型——这些都是为了你的软件工程师同事，他们将在你之后处理这些代码库，而不是帮助你实现带来价值的目的。（请注意，带来价值也可以意味着构建一个不会崩溃的可伸缩的解决方案，这要求代码写的至少像点样。）</p>
<h2 id="你需要和不称职的人打交道">你需要和不称职的人打交道</h2>
<p>大多数工作环境中都会有不称职的人。不是指他们就是你的经理，他们可以是提供API的合作伙伴公司的经理，也可以是客户的某些高管。和不称职的人协作是非常令人沮丧和疲劳的。他们创造了一种有害和低效的工作环境。他们花了太多时间来做决定，或者做出了糟糕的决定，从而给团队和项目带来了负面影响。这导致了持续的延迟和返工，浪费了宝贵的时间和资源。</p>
<p>我花了相当多的时间来寻找有效的方法来处理这些事情，同时又不会成为一个混蛋。我认为大学应该传授此技能。</p>
<p>我发现了一种有效的方法就是不管别人怎样，都要专注于高效。我试图寻找其他可能更有效的解决方案，以及避免无效的人参与。比如，记录一切也是很有帮助的。这可以提供具体的证据，证明他们的不称职对项目进程的影响。</p>
<p>最终，应对不称职的最好方法是积极主动，找到绕过他们局限性的方法。这可能涉及：</p>
<ol>
<li class="lvl-3">
<p>寻求额外的资源或支持。</p>
</li>
<li class="lvl-3">
<p>想办法把任务委派给更有能力的人。</p>
</li>
<li class="lvl-3">
<p>实现故障保护和回退机制，这样事情不会卡在你这边。</p>
</li>
<li class="lvl-3">
<p>以面对面的方式告诉对方，他们阻碍了这个进程。</p>
</li>
<li class="lvl-3">
<p>再说一遍，没必要当混蛋。</p>
</li>
</ol>
<h2 id="大部分时间都在与不确定性打交道">大部分时间都在与不确定性打交道</h2>
<p>与人打交道很难。处理不确定性也很难。与不确定的人打交道更难。这就是你作为一个软件开发人员要做的。</p>
<p>人们并不总是知道他们想要什么，有时他们没有意识到一个简单的改变可能是非常复杂的——“哦，你的意思是我们不能仅仅是变更支付供应商？而是整个信用卡付款流程的改变，对吧？”</p>
<p>他们在大学里对你说的一个大谎言是，你的项目经理会给你适当的、结构化的、简单的指令，告诉你需要做什么，然后你编写代码。“画一个曼德勃罗特集”或“渲染一个环境遮蔽的Rabbit mesh”。在一天结束的时候，你有了一个解决方案，你和你的经理击掌，然后微笑着回家。</p>
<p>实际上会发生的是，你的产品经理会给你一个任务的粗略轮廓，“我们需要一些东西来把我们从A点带到B点，但我们还没有任何设计，第三方集成也不会立马交付，除非我们告诉他们我们想要什么，X老板希望它是红色的，Y老板希望它是绿色的。”这就是软件工程师的“真正工作”开始的地方——收集需求，弄清楚需要做什么。</p>
<p>需求收集在编程中可不是简简单单的。这没有写代码那么有趣。但作为程序员，这需要你花费大量的时间，因为它需要与人而不是机器合作——打电话给提供第三方集成的机构，并与他们的开发人员交谈，以了解什么是可行的，什么是不可行的。与利益相关方坐下来，告诉他们，他们的想法没有意义，我们可以这样做，不能那样做。</p>
<p>编写第一行代码可能需要数周时间。你需要弄清楚需求，然后弄清楚它需要放在哪里，然后弄清楚它需要如何构建，然后弄清楚它可能会在哪里出错，然后才真正开始编写第一行代码。</p>
<h2 id="假设所有东西都有bug">假设所有东西都有bug</h2>
<p>这是很多开发人员对于信任的一个普遍误解：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>你很少充分信任你的代码，因为你知道你也是人，也会犯错误。</p>
</li>
<li class="lvl-2">
<p>你使用的第三方库可能会有bug，但它们是由比你更有能力的人编写的，对吧？</p>
</li>
<li class="lvl-2">
<p>标准操作系统库不应该有任何bug，对吧？它们是由更聪明的人写的。</p>
</li>
<li class="lvl-2">
<p>CPU/硬件应该永远不会出错，对吧？他们花了好几年研发这个东西；它不应该坏。</p>
</li>
<li class="lvl-2">
<p>供电是不应该中断的啊。哎。</p>
</li>
</ul>
<p>但事实是——我们永远不能完全相信我们的代码、库甚至硬件不会在某些时候中断；相反，我们需要假设它会。即使是聪明人也会犯糊涂。</p>
<p>如果你查看任何流行库（操作系统或应用程序级别）的GitHub issue，你会看到大量未定义的操作等待被修复。天啊，我的Linux机器有多少次因为分段故障崩溃了？这太疯狂了。</p>
<p>通过假设一切都可能发生故障或有bug，我们可以采取措施来预防或减轻潜在的问题，这最终有助于确保系统的可靠性和稳定性。</p>
<h2 id="这不是一份理想的工作">这不是一份理想的工作</h2>
<p>你的大学或培训机构都会告诉你，一旦你开始工作，你将拥有的美好生活是什么。但那都只是一个空洞的承诺。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>这是一项艰苦的工作。你一天大部分时间都坐在电脑前面。</p>
</li>
<li class="lvl-2">
<p>工作和生活很难平衡。一般其他职业是，一天的工作在18:00结束，然后就可以完全忘记工作。但在这不是的。你很可能一直在线并查看代码，即使是在深夜。</p>
</li>
<li class="lvl-2">
<p>你很少会有时间做自己喜欢的事情。而且通常情况下，这是一项需要完成的乏味工作。</p>
</li>
<li class="lvl-2">
<p>职业发展机会有限。即使你是一名优秀的员工，在公司里也可能没有提升的空间。</p>
</li>
<li class="lvl-2">
<p>压力的环境。最后期限、bug和满足客户期望的压力都会导致压力倍增。</p>
</li>
<li class="lvl-2">
<p>远程工作可能导致孤立。根据公司和团队结构的不同，软件工程师可能会长时间独处（不包括视频通话），导致缺乏真正的社交互动。</p>
</li>
<li class="lvl-2">
<p>工作保障有限。随着技术的不断发展，软件工程师可能会面临被更新、更高效的技术取代的风险。</p>
</li>
</ul>
<h2 id="美学是教不来的">美学是教不来的</h2>
<p>大学课程教会了我们做出优秀代码的基础知识，但是软件开发中的真正美学是无法在课堂上教授的。</p>
<p>软件开发中的美学是指代码的整体外观和感觉。关键在于它是否易于阅读、理解和维护。美观的代码是干净、有组织并遵循逻辑模式的代码。这是一种让你在看到它的时候感觉优雅的代码。或者在糟糕的时候让你畏缩。</p>
<p>不幸的是，美学不能在一个学期的课程中教授。它是通过经验、阅读大量好代码和维护坏代码获得的。</p>
<h2 id="即使你不想给出估算，但还是会有人问你">即使你不想给出估算，但还是会有人问你</h2>
<p>经理们都喜欢数字、估算，以及用写在餐巾上的想法来要求估算。这就是现实世界的运作方式——企业有一些利润目标，但在批准立项之前，他们需要了解成本。</p>
<p>在大学里很难传授这一点，因为准确性高度取决于你构建系统的经验。你多年来解决的问题越多，就越容易估计未来的工作。</p>
<p>我不打算讨论做估算的最佳方法；有很多方法可以做到。但我要说的是，估算是企业唯一能理解的东西。如果你开始谈论“我们有长期计划，但我不知道我们什么时候能完成”，那么在这种前提下，公司很难生存。</p>
<p>在Mindnow，我们通常会粗略地估算整个项目，以估算需要分配多少预算——这是长期的优先事项。然后，我们开始基于冲刺的计划，整个团队讨论、确定优先级并提交到短期可交付成果中，使我们更接近长期优先级。</p>
<h2 id="并非所有的会议都是无用的">并非所有的会议都是无用的</h2>
<p>既然，软件工程师的工作并不是花大量时间写代码，那时间都去哪了呢？答案——会议。</p>
<p>会议的目的是确保一切进展顺利，按时进行。他们让人们围绕一个共同的目标保持一致，并让每个人都走上正轨。市场营销部门知道有些东西正在开发中，他们可以为功能的最终发布做准备。项目经理了解开发人员的工作方向，并在需要时进行微小的修正。客户支持带来了最终用户的反馈。质量保证部门分享他们发现的问题。管理层分享利益相关方的最新情况。</p>
<p>所有这些都是相互关联的，而会议是信息共享的场所。作为一名软件工程师，需要对这种信息共享的一部分负有责任，因此阻碍它是不负责任的。虽然你可能不喜欢这样，但是必须共享信息以保持系统的效率。</p>
<h2 id="结论">结论</h2>
<p>如果你正在考虑从事软件工程师的职业，请准备好面对这些事实并拥抱这成长的机会。你不太可能给世界带来多么有意义的改变，但说到底，这只是一份工作，你可以通过其他方式做出有意义的贡献。</p>
<p>最重要的是——不忘初心，享受工作。</p>
<blockquote>
<p>原文链接🔗：<a href="https://mp.weixin.qq.com/s/5E6mbgSZggSLyxMGx2Qy5w">https://mp.weixin.qq.com/s/5E6mbgSZggSLyxMGx2Qy5w</a></p>
</blockquote>
]]></content>
      <tags>
        <tag>经验分享</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习算法备忘单</title>
    <url>/post/38dd791a.html</url>
    <content><![CDATA[<p>机器学习（ML）是人工智能（AI）和计算机科学的一个子领域，主要是利用数据和算法来模仿人的学习方式，逐步提高其准确性。使用这个树状图作为指南，以确定使用哪种ML算法来解决你的AI问题。</p>
<div align=center>
    <img src=https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/640.png width=90%></img>
<div style="display: inline-block; color: #999; padding: 2px;">图片来源：LatinX 在 AI™ 中的机器学习算法备忘单</div><br><br>
</div>
<p>如果你想知道在不同的应用程序中使用哪些机器学习算法，或者你是一个开发者，同时为你试图解决的问题寻找一种方法，请继续阅读下文，并以这些步骤作为指导。</p>
<p><ins>无监督学习</ins>使用未标记的信息数据，这样机器应该在没有指导的情况下根据模式、相似性和差异来工作。</p>
<p>另一方面，<ins>有监督学习</ins>有一个 “老师” 存在，他负责通过标记数据来训练机器工作。接下来，机器会收到一些示例，使其能够产生正确的结果。</p>
<p>但是对于这些类型的学习，有一种混合的方法，这种<ins>半监督学习</ins>适用于有标签和无标签的数据。这种方法使用一个极小的标记数据集来训练和标记其余数据，并进行相应的预测，最后给出问题的解决方案。</p>
<p>首先，你需要知道你所处理的维数，它意味着你的问题中输入的数量（也被称为特征）。如果你正在处理一个大数据集或许多特征，你可以选择降维算法。</p>
<h1>无监督学习：降维</h1>
<p>数据集合中的大量维度可能会对机器学习算法的性能产生重大影响。“维度诅咒” 是一个用来描述大维度可能造成的麻烦的术语，例如，聚类中的 “距离聚集（Distance Concentration）” 问题，即随着数据维度的增加，不同的数据点会有相同的值。</p>
<p>最小化训练数据中输入变量数量的技术被称为 “降维”。</p>
<p>现在你需要熟悉<ins>特征提取</ins>和<ins>特征选择</ins>的概念，以便继续学习。</p>
<p>将原始数据转化为可以处理的数字特征，同时保留原始数据集的信息，这一过程被称为<ins>特征提取</ins>。它比直接将机器学习应用于原始数据产生更好的结果。</p>
<p>它用于三种已知的降维算法，包括主成分分析、奇异值分解和线性判别分析，但你需要清楚地知道你想用哪种工具来寻找模式或从数据中推断出新的信息。</p>
<p>如果你不希望合并数据中的变量，而是想通过只保留重要的特征来去除不需要的特征，那么你可以使用主成分分析算法。</p>
<h2 id="PCA（主成分分析）">PCA（主成分分析）</h2>
<p>主成分分析是一种降低数据集维数的数学算法，在保留大部分信息的同时简化变量的数量。这种以准确性换取简单性的方法被广泛用于在大型数据集中寻找模式。</p>
<div align=center>
    <img src=https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/机器学习算法备忘单_1.png width=70%></img>
<div style="display: inline-block; color: #999; padding: 2px;">图片来源：https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/</div><br><br>
</div>
<p>在线性连接方面，它在有大量数据存在的情况下有着广泛的应用，如媒体编辑、统计质量控制、投资组合分析，以及人脸识别、图像压缩等许多应用。</p>
<p>另外，如果你想要一个通过组合你正在使用的数据的变量来工作的算法，简单的PCA可能不是你使用的最佳工具。接下来，你可以有一个概率模型或一个非概率模型。概率数据是涉及到随机选择的数据，是大多数科学家的首选，可以得到更准确的结果。而非概率数据不涉及这种随机性。</p>
<p>如果你正在处理非概率数据，你应该使用奇异值分解算法。</p>
<h2 id="SVD（奇异值分解）">SVD（奇异值分解）</h2>
<p>在机器学习领域，SVD允许数据被转化为一个可以轻松区分类别的空间。这种算法将一个矩阵分解为三个不同的矩阵。例如，在图像处理中，使用数量减少的矢量来重建与原始图像非常接近的图片。</p>
<div align=center>
    <img src=https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/机器学习算法备忘单_2.png width=100%></img>
<div style="display: inline-block; color: #999; padding: 2px;">使用给定数量的组件压缩图像</div><br><br>
</div>
<p>与PCA算法相比，两者都可以对数据进行降维处理。但PCA跳过了不太重要的成分，而SVD只是把它们变成特殊的数据，表示为三个不同的矩阵，更容易操作和分析。</p>
<p>当涉及到概率方法时，对于更抽象的问题，最好使用线性判别分析算法。</p>
<h2 id="LDA（线性判别分析）">LDA（线性判别分析）</h2>
<p>线性判别分析（LDA）是一种分类方法，在这种方法中，先前已经确定了两个或更多的组，根据其特征将新的观察结果归为其中一个。</p>
<p>它不同于PCA，因为LDA发现了一个优化组可分离性的特征子空间，而PCA忽略了类标签，专注于捕捉数据集的最高方差方向。</p>
<p>该算法使用贝叶斯定理，这是一个概率定理，用于根据一个事件与另一个事件的关系来确定其发生的可能性。</p>
<p>它经常被用于人脸识别、客户识别和医学领域，以识别病人的疾病状况。</p>
<div align=center>
    <img src=https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/机器学习算法备忘单_3.png width=900%></img>
</div>
从UMIST数据库中随机选取的五个主体（类）的170张人脸图像在（a）基于PCA的子空间，（b）基于D-LDA的子空间，以及（c）基于DF-LDA的子空间的分布。
<p>资料来源：Face recognition using LDA-based algorithms</p>
<p><a href="https://www.researchgate.net/publication/5613964">https://www.researchgate.net/publication/5613964</a></p>
<p>下一步是选择你是否希望你的算法有响应，这意味着你要开发一个基于标记数据的预测模型来教导你的机器。如果你愿意使用非标签数据，你可以使用聚类技术，这样你的机器就可以在没有指导的情况下工作，搜索相似性。</p>
<p>另一方面，选择相关特征（变量、预测因子）的子集用于模型创建的过程被称为<ins>特征选择</ins>。它有助于简化模型，使研究人员和用户更容易理解它们，以及减少训练周期和避免维度诅咒。</p>
<p>它包括聚类法、回归法和分类法。</p>
<h1>监督学习：聚类</h1>
<p>聚类是一种分离具有相似特征的群体并将其分配到群组的技术。</p>
<p>如果你正在寻找一种分层的算法：</p>
<h2 id="Hierarchical-Clustering（层次聚类）">Hierarchical Clustering（层次聚类）</h2>
<p>这种类型的聚类是机器学习中最流行的技术之一。层次聚类协助一个组织对数据进行分类，以确定相似性，以及不同的分组和特征，从而使其定价、商品、服务、营销信息和其他方面的业务有的放矢。它的层次结构应显示出类似于树状数据结构的数据，即所谓的树状图。有两种方法对数据进行分组：聚类和分化。</p>
<p><ins>聚合式聚类</ins>是一种 “自下而上” 的方法。换句话说，每个项目首先被认为是一个单元素集群（叶子）。在该方法的每个阶段，最具可比性的两个集群被连接成一个新的更大的集群（结点）。这种方法反复进行，直到所有的点都属于单个大簇（根）。</p>
<p><ins>分化聚类</ins>以一种 “自上而下” 的方式工作。它从根部开始，所有项目都分组在一个集群中，然后在每个迭代阶段将最多的项目分成两个。迭代程序直到所有的项目都在他们的组中。</p>
<p>如果你不寻找分层解决方案，则必须确定你的方法是否需要指定要使用的集群数量。如果你不需要定义，你可以利用基于密度的有噪声的应用程序空间聚类算法。</p>
<h2 id="DBSCAN（基于密度的有噪声的应用程序空间聚类法）">DBSCAN（基于密度的有噪声的应用程序空间聚类法）</h2>
<p>当涉及到任意形状的聚类或检测异常值时，最好使用基于密度的聚类方法。DBSCAN是一种检测那些任意形状的聚类和有噪声的聚类方法，它根据两个参数：eps和minPoints将彼此接近的点分组。</p>
<p>eps告诉我们两个点之间需要有多大的距离才能被视为一个集群。而minPoints是创建一个集群的最小点数。</p>
<p>我们在分析Netflix服务器的异常值时使用了这种算法。流媒体服务运行着数以千计的服务器，通常只有不到百分之一的服务器能够变得不健康，这会降低流媒体的性能。真正的问题是这个问题不容易被发现，为了解决这个问题，Netflix使用DBSCAN指定一个要监测的指标，然后收集数据，最后传递给算法来检测服务器的异常值。</p>
<div align=center>
    <img src=https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/机器学习算法备忘单_4.png width=90%></img>
<div style="display: inline-block; color: #999; padding: 2px;">资料来源：Tracking down the Villains: Outlier Detection at Netflix</div><br><br>
</div>
<p>日常使用可以是电子商务向客户推荐产品。对用户之前购买过的产品数据应用DBSCAN。</p>
<p>如果你需要指定聚类的数量，有三种现有的算法可供使用，包括K-Modes、K-Means和高斯混合模型。接下来，你需要知道是否要使用分类变量，这是一种离散变量，通过对观察值进行分组来捕捉定性的后果。如果你要使用它们，你可以选择K-Modes。</p>
<h2 id="K-Modes">K-Modes</h2>
<p>这种方法被用来对分类变量进行分组。我们确定这些类型的数据点之间的总不匹配度。我们的数据点之间的差异越少，它们就越相似。</p>
<p>K-Modes和K-Means之间的主要区别是：对于分类数据点，我们不能计算距离，因为它们不是数字值。</p>
<p>这种算法被用于文本挖掘应用、文档聚类、主题建模（每个聚类组代表一个特定的主题）、欺诈检测系统和市场营销。</p>
<p>对于数值型数据，你应该使用K-Means聚类。</p>
<h2 id="K-Means">K-Means</h2>
<p>数据被聚类为k个组，其方式是同一聚类中的数据点是相关的，而其他聚类中的数据点则相距较远。这种距离经常用欧几里得距离来衡量。换句话说，K-Means算法试图最小化聚类内的距离，最大化不同聚类之间的距离。</p>
<p>搜索引擎、消费者细分、垃圾邮件检测系统、学术表现、缺陷诊断系统、无线通信和许多其他行业都使用K-Means聚类。</p>
<p>如果预期的结果是基于概率的，那么你应该使用高斯混合模型。</p>
<h2 id="GMM（高斯混合模型）">GMM（高斯混合模型）</h2>
<p>这种方法意味着存在许多高斯分布，每个高斯分布代表一个集群。该算法将确定每个数据点属于给定批次数据的每个分布的概率。</p>
<p>GMM与K-Means不同，因为在GMM中，我们不知道一个数据点是否属于一个指定的聚类，我们使用概率来表达这种不确定性。而K-Means方法对一个数据点的位置是确定的，并开始在整个数据集上迭代。</p>
<p>高斯混合模型经常被用于信号处理、语言识别、异常检测和音乐的流派分类。</p>
<p>在使用标记数据来训练机器的情况下，首先，你需要指定它是否要预测数字，这种数字预测将有助于算法解决问题。如果是这样的话，你可以选择回归算法。</p>
<h1>监督学习：回归</h1>
<p>回归是一种机器学习算法，其结果被预测为一个连续的数值。这种方法通常用于银行、投资和其他领域。</p>
<p>在这里，你需要对速度和准确性做出取舍。如果你正在寻找速度，你可以使用决策树算法或线性回归算法。</p>
<h2 id="决策树">决策树</h2>
<p>决策树是一个类似树形数据结构的流程图。在这里，数据根据一个给定的参数被连续分割。每个参数允许在一个树节点中，而整个树的结果位于叶子中。有两种类型的决策树。</p>
<p>决策树是一个类似树形数据结构的流程图。在这里，数据根据一个给定的参数被连续分割。每个参数允许在一个树节点中，而整个树的结果位于叶子中。有两种类型的决策树。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>分类树（是/否类型），这里的决策变量是分类的。</p>
</li>
<li class="lvl-2">
<p>回归树（连续数据类型），这里的决策或结果变量是连续的。</p>
</li>
</ul>
<p>当特征和输出变量之间存在复杂的相互作用时，决策树就会派上用场。当存在缺失的特征，类别和数字特征的混合，或特征大小的巨大差异时，与其他方法相比，它们的表现更好。</p>
<p>该算法用于提高促销活动的准确性、欺诈检测以及患者严重或可预防疾病的检测。</p>
<h2 id="线性回归">线性回归</h2>
<p>基于一个给定的自变量，这种方法预测因变量的值。因此，这种回归方法决定了输入（自变量）和输出（因变量）之间是否存在线性联系。这也是线性回归这一术语的由来。</p>
<p>线性回归非常适合于那些特征和输出变量具有线性关系的数据集。</p>
<p>它通常用于预测（这对小公司了解销售效果特别有用），了解广告支出和收入之间的联系，以及在医疗行业了解药物剂量和病人血压之间的相关性。</p>
<p>另外，如果你的算法需要准确性，你可以使用以下三种算法。神经网络、梯度提升树和随机森林。</p>
<h2 id="神经网络">神经网络</h2>
<p>需要一个神经网络来学习特征和目标之间复杂的非线性关系。它是一种模拟人脑中神经元工作的算法。有几种类型的神经网络，包括香草神经网络（只处理结构化数据），以及循环神经网络和卷积神经网络，它们都可以处理非结构化数据。</p>
<p>当你有大量的数据（和处理能力），并且准确性对你很重要时，你几乎肯定会利用神经网络。</p>
<p>这种算法有很多应用，例如释义检测、文本分类、语义解析和问答。</p>
<h2 id="Gradient-Boosting-Tree（梯度提升树）">Gradient Boosting Tree（梯度提升树）</h2>
<p>梯度提升树是一种将不同树的输出合并进行回归或分类的方法。这两种监督学习都结合了大量的决策树，以减少每棵树单独面对的过拟合的危险（一种统计建模错误，当一个函数与少量数据点过于紧密匹配时，就会出现这种情况，使得模型的预测能力下降）。这种算法采用了<em>Boosting</em>，它需要连续组合弱学习器(通常是只有一次分裂的决策树，称为决策树桩)，以便每棵新树都纠正前一棵树的错误。</p>
<p>当我们希望减少偏差误差时，也就是模型的预测与目标值之间的差异，我们通常采用梯度提升算法。</p>
<p>当数据的维度较少，基本的线性模型表现不佳，可解释性并不重要，而且没有严格的延迟限制时，梯度提升算法是最有利的。</p>
<p>它被用在很多研究中，比如基于大师级运动员动机的性别预测算法，使用梯度提升决策树，探索他们基于心理维度预测性别的能力，评估参加大师级运动的原因作为统计方法。</p>
<h2 id="随机森林">随机森林</h2>
<p>随机森林是一种解决回归和分类问题的方法。它利用了集成学习，这是一种通过结合几个分类器来解决复杂问题的技术。</p>
<p>它由许多决策树组成，其中每一个决策树的结果都会以平均或平均决策的方式得出最终结果。树的数量越多，结果的精确度就越高。</p>
<p>当我们有一个巨大的数据集并且可解释性不是一个关键问题时，随机森林是合适的，因为随着数据集的增大，它变得越来越难以把握。</p>
<p>这种算法被用于股票市场分析、医疗领域的病人诊断、预测贷款申请人的信用度，以及欺诈检测。</p>
<p>对于非数字预测算法，你可以选择分类方法而不是回归。</p>
<h1>监督学习：分类</h1>
<p>与回归方法一样，你选择的结果是偏向于速度还是准确性。</p>
<p>如果你在寻找准确性，你不仅可以选择核支持向量机，还可以使用之前提到的其他算法，如神经网络、梯度提升树和随机森林。现在，让我们来介绍一下这个新算法。</p>
<h2 id="Kernel-Support-Vector-Machine（核支持向量机）">Kernel Support Vector Machine（核支持向量机）</h2>
<p>在支持向量机模型中，通常使用核技术来连接线性和非线性。为了理解这一点，有必要知道SVM方法学习如何通过形成决策边界来分离不同的组。</p>
<p>但是，当我们在一个维度较高的数据集面前，而且成本昂贵时，建议使用这种核方法。它使我们能够在原始特征空间中工作，而不必在高维空间中计算数据的坐标。</p>
<p>它主要用于文本分类问题，因为大多数问题都可以被线性分离。</p>
<p>当需要速度的时候，我们需要看看我们要采用的技术是否是可解释的，这意味着它可以解释你的模型中从头到尾发生了什么。在这种情况下，我们可能会使用决策树算法或Logistic回归算法。</p>
<h2 id="Logistic-Regression（逻辑回归）">Logistic Regression（逻辑回归）</h2>
<p>当因变量是分类的时候，就会使用Logistic回归。通过概率估计，它有助于理解因变量和一个或多个自变量之间的联系。</p>
<p>有三种不同类型的Logistic回归。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>二元逻辑回归，响应只有两个可能的值。</p>
</li>
<li class="lvl-2">
<p>多项式Logistic回归，三个或更多的结果，没有顺序。</p>
</li>
<li class="lvl-2">
<p>有序逻辑回归，三个或更多的类别，有顺序。</p>
</li>
</ul>
<p>逻辑回归算法在酒店预订中被广泛使用，它（通过统计研究）向你展示了你在预订中可能想要的选项，如酒店房间、该地区的一些行程等等。</p>
<p>如果你只对问题的输入和输出感兴趣，你可以检查你所处理的数据是否太大。如果数量很大，你可以使用线性支持向量机。</p>
<h2 id="Linear-Support-Vector-Machine（线性支持向量机）">Linear Support Vector Machine（线性支持向量机）</h2>
<p>线性SVM用于线性可分离的数据。它在具有不同变量的数据（线性可分离数据）中工作，这些变量可以用一条简单的直线（线性SVM分类器）来分离。这条直线代表了用户的行为或通过既定问题的结果。</p>
<p>由于文本通常是线性可分离的，并且有很多特征，因此线性SVM是用于其分类的最佳选择。</p>
<p>在我们的下一个算法中，如果数据量大或者不大，你都可以使用它。</p>
<h2 id="Naive-Bayes（朴素贝叶斯）">Naïve Bayes（朴素贝叶斯）</h2>
<p>这种算法是基于贝叶斯定理的。它包括通过对象的概率进行预测。它被称为Naïve（朴素），是因为它假设一个特征的出现与其他特征的出现无关。</p>
<p>这种方法深受欢迎，因为它甚至可以超越最复杂的分类方法。此外，它构造简单，可迅速建立。</p>
<p>由于其易于使用和高效，它被用来做实时决策。与此同时，Gmail使用这种算法来知道一封邮件是否是垃圾邮件。</p>
<p>Gmail垃圾邮件检测选择一组词或 “标记” 来识别垃圾邮件（这种方法也用于文本分类，它通常被称为词袋）。接下来，他们使用这些tokens（令牌），将其与垃圾邮件和非垃圾邮件进行比较。最后，使用Naïve Bayes算法，他们计算出该邮件是否是垃圾邮件的概率。</p>
<h1>总结</h1>
<p>我们发现，机器学习是一种被广泛使用的技术，由于它经常发生，因此我们无法识别许多应用。在这篇文章中，我们不仅区分了机器学习的不同方法，还区分了如何根据我们正在处理的数据和我们想要解决的问题来使用它们。</p>
<p>要学习机器学习，你必须具备一些微积分、线性代数、统计学和编程技能的知识。你可以使用不同的编程语言来实现其中一种算法，从Python到C++，以及R语言。这取决于你做出最好的决定，并与你的机器一起开始学习。</p>
<blockquote>
<p>原文链接🔗：<a href="https://medium.com/accel-ai/machine-learning-algorithms-cheat-sheet-990104aaaabc">https://medium.com/accel-ai/machine-learning-algorithms-cheat-sheet-990104aaaabc</a></p>
</blockquote>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机架构设计的8个伟大思想</title>
    <url>/post/842e3940.html</url>
    <content><![CDATA[<h1>面向摩尔定律设计</h1>
<p>计算机设计师的一个永恒的问题就是<ins>摩尔定律（Moore’s law）</ins>。摩尔定律指出，集成电路上可容纳的晶体管数每18~24个月翻一番。摩尔定律是Intel公司创始人之一戈登 摩尔在1965年对集成电路集成度做出的预测。由于计算机设计通常需要几年时间，因此项目结束时芯片的集成度较之项目开始时，很容易翻一番甚至翻两番。像双向飞碟射击运动员一样，计算机体系结构设计师应当预测设计完成时的工艺和技术水平，而不是设计开始时的工艺。</p>
<blockquote>
<p>半导体行业大致按照摩尔定律发展了半个多世纪，对二十世纪后半叶的世界经济增长做出了贡献，并驱动了一系列科技创新、社会改革、生产效率的提高和经济增长。个人电脑、因特网、智能手机等技术改善和创新都离不开摩尔定律的延续。如今单个处理器已经很难适应摩尔定律了（主要包括随着更多晶体管被装入芯片当中，相应会出现电子能量外泄和热量散发的现象），但云计算兴起，算力网络起飞，在某种程度延续了摩尔定律。</p>
</blockquote>
<h1>使用抽象简化设计</h1>
<p>计算机架构师和程序员都必须发明技术来提高自己的生产力，否则设计时间会随着摩尔定律的资源增长而显着延长。提高硬件和软件生产率的主要技术之一是使用++抽象（abstraction）++来表征不同级别的设计。从而，低层将细节隐蔽起来，呈现给高层的只是一个简化的模型。</p>
<blockquote>
<p>抽象思维一直推动着计算机技术不断向前发展，科学技术本身就是现实世界的抽象和演绎：</p>
<p>电路信号-&gt;01二进制-&gt;指令汇编-&gt;高级编程-&gt;模块设计-&gt;框架设计-&gt;单机系统-&gt;分布式系统–&gt;云计算，计算机领域有句名言：“计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决”。</p>
</blockquote>
<h1>加速经常性事件</h1>
<p>使常见情况变得更快往往会比优化罕见情况更有效地提高性能。具有讽刺意味的是，经常性事件通常比罕见情形更简单，因而更易于对其进行优化以提高性能。加速经常性事件意味着设计者需要知道哪些事件是经常发生的， 这要经过仔细的实验与测量过程。</p>
<h1>通过并行提高性能</h1>
<p>从计算诞生开始，计算机架构师就给出了通过并行执行操作来提高性能的设计方案。</p>
<h1>通过流水线提高性能</h1>
<p>在计算机体系结构中，有一种并行技术非常普遍，这种技术有一个特殊的名字： 流水线(pipelining) 。例如，许多西部电影中有这样的场景，在消防车出现之前，人们用“水桶队列”来灭火一一小镇居民们一个接一个排成长队，接力将水桶快速从水源传至火场，而不是让每个人来回奔跑运水灭火。</p>
<h1>通过预测提高性能</h1>
<p>遵循谚语“请求宽恕胜于寻求许可＂ ，下一个伟大的思想是预测( prediction) 。假设预测错误后恢复的代价不大， 并且预测的准确率相对较高，那么通过预测的方式提前开始工作，要比等到确定知道能执行时才启动要效率高一些。</p>
<h1>存储层次</h1>
<p>现如今，计算机价格的很大一部分来自于存储器的开销。存储器对程序执行有很大的影响，其速度影响着程序的性能， 其容量限制了可解决问题的规模。因此，程序员总是希望存储器速度更快、容量更大、价格更便宜。计算机架构师发现，通过<ins>存储层次(hierarchy of memory)</ins> 可以来缓解这些相互矛盾的需求。在存储器层次中，位于顶层的存储器速度最快、容量最小， 但价格最昂贵。反之，处于最底层的存储器速度最慢、容量最大，但价格最便宜。例如，高速缓存可以给程序员造成一种假象，让他们感觉自己所使用的主存既有存储器层次中顶层的高速度，又和底层存储器一样价格便宜、容量大。</p>
<h1>通过冗余提高可靠性</h1>
<p>计算机工作时不仅要快，还要稳定可靠。由于任何物理设备都有可能发生故障，因此可以通过引入冗余组件来提高系统的可靠性 。该组件在系统发生故障时可以替代故障组件并帮助检测故障。例如，牵引式挂车后轴每边都有两个双轮胎，当一个轮胎出问题时，另一个轮胎保证卡车仍然可以继续行使。</p>
]]></content>
      <tags>
        <tag>计算机系统</tag>
      </tags>
  </entry>
</search>
