<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>《Easy RL:强化学习教程》读书笔记01</title>
    <url>/post/61f51f4.html</url>
    <content><![CDATA[<h1>强化学习概述</h1>
<p>强化学习由<ins>智能体（agent）<ins>和</ins>环境（environment）<ins>组成，智能体得到环境中的</ins>状态（state）<ins>后，将根据状态输出一个</ins>动作（action）</ins>，这个动作也是<ins>决策（decision）</ins>，这个动作会在环境中被执行后输出状态和<ins>奖励（reward</ins>），智能体的目标是最大化所获得的奖励。</p>
<h2 id="强化学习和监督学习的区别">强化学习和监督学习的区别</h2>
<p>1、强化学习输入的样本是序列数据，监督学习的样本的独立同分布的。</p>
<p>2、强化学习只能通过不停地尝试去做出最有利的动作，而监督学习是有人类知识引导的。</p>
<p>3、<ins>探索（exploration）<ins>和</ins>利用（exploitation）<ins>是强化学习里面非常核心的问题，<ins>探索</ins>是尝试新动作以获得更高的奖励，但有一定的风险，而</ins>利用</ins>是采取已知的可获最大奖励的动作，所以需要在<ins>探索</ins>和<ins>利用</ins>间进行权衡，这是监督学习中没有的。</p>
<p>4、强化学习中的奖励一般是延迟的，而监督学习是即时的监督。</p>
<h2 id="标准强化学习和深度强化学习的区别">标准强化学习和深度强化学习的区别</h2>
<p><ins>标准强化学习</ins>需要设计特征、进行特征工程并设计价值函数，而<ins>深度强化学习</ins>是一个端到端的训练过程，可直接通过神经网络来拟合价值函数或策略网络。</p>
<p><img src="https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/image-20220816210507041.png" alt="image-20220816210507041"></p>
<h2 id="状态和观测的区别">状态和观测的区别</h2>
<p><ins>状态</ins>是对世界的完整描述，不会隐藏世界的信息。<ins>观测</ins>是对状态的部分描述，可能会遗漏一些信</p>
<p>息。如果状态和观察等价，则该环境是完全可观测的（fully observed），这种情况也被称为<ins>马尔可夫决策过程（Markov decision process，MDP）</ins>。当智能体只能看到部分的观测时，我们就称这个环境是<ins>部分可观测的（partially observed）</ins>，这种情况也被称为<ins>部分可观测马尔可夫决策过程（partially observable Markov decision process, POMDP）</ins>。部分可观测马尔可夫决</p>
<p>策过程可以用一个七元组描述：(<em>S, A, T, R, Ω, O, γ</em>)。其中 <em>S</em> 表示状态空间，为隐变量，<em>A</em> 为动作空间，<em>T</em>(s′|s, a) 为状态转移概率，<em>R</em> 为奖励函数，<em>Ω</em>(<em>o++|++s, a</em>) 为观测概率，<em>O</em> 为观测空间，<em>γ</em> 为折扣因子。</p>
<h1>强化学习智能体</h1>
<h2 id="组成成分">组成成分</h2>
<p>对于一个强化学习智能体，也是<ins>马尔可夫决策过程（Markov decision process）</ins>，它可能有一个或多个如下的组成成分。</p>
<h3 id="策略（policy）">策略（policy）</h3>
<p>智能体会用策略来选取下一步的动作。策略可分为随机性策略和确定性策略。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>++随机性策略（stochastic policy）++也是概率函数，即输入一个状态后输出一个概率，智能体根据该动作概率采取动作。</p>
</li>
<li class="lvl-2">
<p>++确定性策略（deterministic policy）++是智能体直接采取最有可能的动作。</p>
</li>
</ul>
<p>通常情况下，强化学习一般使用随机性策略，随机性策略有很多优点，其具有多样性，能够更好地探索环境。</p>
<h3 id="价值函数（value-function）">价值函数（value function）</h3>
<p>用于评估智能体进入某个状态后，可以对后面的奖励带来多大的影响。价值函数值越大，说明智能体进入这个状态越有利。价值函数里面有一个<ins>折扣因子（discount factor）</ins>，折扣因子越大，越关注未来对现在的影响。常用的价值函数是Q函数，Q 函数里面包含状态和动作两个变量，即未来可以获得奖励的期望取决于当前的状态和当前的动作。</p>
<h3 id="模型（model）">模型（model）</h3>
<p>模型表示智能体对环境的状态进行理解，它决定了环境中世界的运行方式。它由状态转移概率和奖励函数两个部分组成</p>
<h2 id="强化学习智能体的类型">强化学习智能体的类型</h2>
<ul class="lvl-0">
<li class="lvl-2">
<p>++基于价值的智能体（value-based agent）++显式地学习价值函数，隐式地学习它的策略。策略是其从学到的价值函数里面推算出来的。</p>
</li>
<li class="lvl-2">
<p>++基于策略的智能体（policy-based agent）++直接学习策略，我们给它一个状态，它就会输出对应动作的概率。基于策略的智能体并没有学习价值函数。</p>
</li>
<li class="lvl-2">
<p>++演员-评论员智能体（actor-critic agent）++把策略和价值函数都学习了，然后通过两者的交互得到最佳动作。</p>
</li>
</ul>
<h3 id="基于策略和基于价值的强化学习方法有什么区别">基于策略和基于价值的强化学习方法有什么区别?</h3>
<p>在基于策略的强化学习方法中，智能体会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。而在基于价值的强化学习方法中，智能体不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。基于价值迭代的方法只能应用在不连续的、离散的环境下（如围棋或某些游戏领域），对于动作集合规模庞大、动作连续的场景（如机器人控制领域），其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作）。基于价值的强化学习算法有 Q 学习（Q-learning）、Sarsa等，而基于策略的强化学习算法有策略梯度（Policy Gradient，PG）算法等。</p>
<h3 id="有模型强化学习和免模型强化学习有什么区别？">有模型强化学习和免模型强化学习有什么区别？</h3>
<p>有模型强化学习是指根据环境中的经验，构建一个虚拟世界，同时在真实环境和虚拟世界中学习；免模型强化学习是指不对环境进行建模，直接与真实环境进行交互来学习到最优策略。</p>
<p>免模型强化学习通常属于数据驱动型方法，需要大量的采样来估计状态、动作及奖励函数，从而优化动作策略。相比之下，有模型的深度强化学习可以在一定程度上缓解训练数据匮乏的问题，因为智能体可以在虚拟世界中进行训练。免模型学习的泛化性要优于有模型强化学习，原因是有模型强化学习算需要对真实环境进行建模，并且虚拟世界与真实环境之间可能还有差异，这限制了有模型强化学习算法的泛化性。</p>
<p>目前，大部分深度强化学习方法都采用了免模型强化学习，这是因为：免模型强化学习更为简单、直观且有丰富的开源资料</p>
<blockquote>
<p>第一章习题链接：<a href="https://datawhalechina.github.io/easy-rl/#/chapter1/chapter1_questions&amp;keywords">https://datawhalechina.github.io/easy-rl/#/chapter1/chapter1_questions&amp;keywords</a></p>
</blockquote>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>《Easy RL:强化学习教程》读书笔记02</title>
    <url>/post/9f16004e.html</url>
    <content><![CDATA[<h1>策略迭代</h1>
<p>策略迭代由两个步骤组成：策略评估和策略改进（policy improvement）。第一个步骤是策略评估，当前我们在优化策略 \pi<em>π</em>，在优化过程中得到一个最新的策略。我们先保证这个策略不变，然后估计它的价值，即给定当前的策略函数来估计状态价值函数。 第二个步骤是策略改进，得到 状态价值函数后，我们可以进一步推算出它的 Q 函数。得到 Q 函数后，我们直接对 Q 函数进行最大化，通过在 Q 函数做一个贪心的搜索来进一步改进策略。这两个步骤一直在迭代进行。</p>
<h1>价值迭代</h1>
<p>价值迭代做的工作类似于价值的反向传播，每次迭代做一步传播，所以中间过程的策略和价值函数 是没有意义的。而策略迭代的每一次迭代的结果都是有意义的，都是一个完整的策略。如图所示为一个可视化的求最短路径的过程，在一个网格世界中，我们设定了一个终点，也就是左上角的点。不管我们在哪一个位置开始，我们都希望能够到达终点（实际上这个终点在迭代过程中是不必要的，只是为了更好地演示）。价值迭代的迭代过程像是一个从某一个状态（这里是我们的终点）反向传播到其他各个状态的过程，因为每次迭代只能影响到与之直接相关的状态。</p>
<p><img src="https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/2.52.png" alt="2.52"></p>
<h1>策略迭代和价值迭代的区别</h1>
<p>我们再来对比策略迭代和价值迭代，这两个算法都可以解马尔可夫决策过程的控制问题。策略迭代分两步。首先进行策略评估，即对当前已经搜索到的策略函数进行估值。得到估值后，我们进行策略改进，即把 Q 函数算出来，进行进一步改进。不断重复这两步，直到策略收敛。价值迭代直接使用贝尔曼最优方程进行迭代，从而寻找最佳的价值函数。找到最佳价值函数后，我们再提取最佳策略。</p>
<h1>马尔可夫决策过程中的预测和控制总结</h1>
<p>我们使用动态规划算法来解马尔可夫决策过程里面的预测和控制，并且采取不同的贝尔曼方程。对于预测问题，即策略评估的问题，我们不停地执行贝尔曼期望方程，这样就可以估计出给定的策略，然后得到价值函数。对于控制问题，如果我们采取的算法是策略迭代，使用的就是贝尔曼期望方程；如果我们采取的算法是价值迭代，使用的就是贝尔曼最优方程。</p>
<p><img src="https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/image-20220821172111216.png" alt="image-20220821172111216"></p>
<h1>动态规划方法和蒙特卡洛方法的差异</h1>
<p>蒙特卡洛方法通过一个回合的经验平均回报（实际得到的奖励）来进行更新。蒙特卡洛方法相比动态规划方法是有一些优势的。首先，蒙特卡洛方法适用于环境未知的情况，而动态规划是有模型的方法。蒙特卡洛方法只需要更新一条轨迹的状态，而动态规划方法需要更新所有的状态。状态数量很多的时候（比如 100 万个、200 万个），我们使用动态规划方法进行迭代，速度是非常慢的。这也是基于采样的蒙特卡洛方法相对于动态规划方法的优势。</p>
<h1>时序差分方法</h1>
<p>时序差分是介于蒙特卡洛和动态规划之间的方法，它是免模型的，不需要马尔可夫决策过程的转移矩阵和奖励函数。此外，时序差分方法可以从不完整的回合中学习，并且结合了自举的思想。</p>
<h1>时序差分方法和蒙特卡洛方法的差异</h1>
<p>（1）时序差分方法可以在线学习（online learning），每走一步就可以更新，效率高。蒙特卡洛方法必须等游戏结束时才可以学习。</p>
<p>（2）时序差分方法可以从不完整序列上进行学习。蒙特卡洛方法只能从完整的序列上进行学习。</p>
<p>（3）时序差分方法可以在连续的环境下（没有终止）进行学习。蒙特卡洛方法只能在有终止的情况下学习。</p>
<p>（4）时序差分方法利用了马尔可夫性质，在马尔可夫环境下有更高的学习效率。蒙特卡洛方法没有假设环境具有马尔可夫性质，利用采样的价值来估计某个状态的价值，在不是马尔可夫的环境下更加有效。</p>
<p>例子：时序差分方法是指在不清楚马尔可夫状态转移概率的情况下，以采样的方式得到不完整的状态序列，估计某状态在该状态序列完整后可能得到的奖励，并通过不断地采样持续更新价值。蒙特卡洛则需要经历完整的状态序列后，再来更新状态的真实价值。例如，我们想获得开车去公司的时间，每天上班开车的经历就是一次采样。假设我们今天在路口 A 遇到了堵车，时序差分方法会在路口 A 就开始更新预计到达路口 B、路口 C <em>· · · · · ·</em> ，以及到达公司的时间；而蒙特卡洛方法并不会立即更新时间，而是在到达公司后，再更新到达每个路口和公司的时间。时序差分方法能够在知道结果之前就开始学习，相比蒙特卡洛方法，其更快速、灵活。</p>
<h1>动态规划方法、蒙特卡洛方法以及时序差分方法的自举和采样</h1>
<p>自举是指更新时使用了估计。蒙特卡洛方法没有使用自举，因为它根据实际的回报进行更新。动态规划方法和时序差分方法使用了自举。</p>
<p>采样是指更新时通过采样得到一个期望。蒙特卡洛方法是纯采样的方法。动态规划方法没有使用采样，它是直接用贝尔曼期望方程来更新状态价值的。时序差分方法使用了采样。时序差分目标由两部分组成，一部分是采样，一部分是自举。</p>
<p><img src="https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/image-20220821213946200.png" alt="image-20220821213946200"></p>
<h1>Q学习和Sarsa的区别</h1>
<ul class="lvl-0">
<li class="lvl-2">
<p>Sarsa 是一个典型的同策略算法，它只用了一个策略 π\pi<em>π</em>，它不仅使用策略 π\pi<em>π</em> 学习，还使用策略 π\pi<em>π</em> 与环境交互产生经验。 如果策略采用 ε\varepsilon<em>ε</em>-贪心算法，它需要兼顾探索，为了兼顾探索和利用，它训练的时候会显得有点“胆小”。它在解决悬崖行走问题的时候，会尽可能地远离悬崖边，确保哪怕自己不小心探索了一点儿，也还是在安全区域内。此外，因为采用的是 ε\varepsilon<em>ε</em>-贪心 算法，策略会不断改变（ε\varepsilon<em>ε</em> 值会不断变小），所以策略不稳定。</p>
</li>
<li class="lvl-2">
<p>Q学习是一个典型的异策略算法，它有两种策略————目标策略和行为策略，它分离了目标策略与行为策略。Q学习可以大胆地用行为策略探索得到的经验轨迹来优化目标策略，从而更有可能探索到最佳策略。行为策略可以采用 ε\varepsilon<em>ε</em>-贪心 算法，但目标策略采用的是贪心算法，它直接根据行为策略采集到的数据来采用最佳策略，所以 Q学习 不需要兼顾探索。</p>
</li>
<li class="lvl-2">
<p>我们比较一下 Q学习 和 Sarsa 的更新公式，就可以发现 Sarsa 并没有选取最大值的最大化操作。因此，Q学习是一个非常激进的方法，它希望每一步都获得最大的利益；Sarsa 则相对较为保守，它会选择一条相对安全的迭代路线。</p>
</li>
</ul>
<blockquote>
<p>第二章习题链接：<a href="https://datawhalechina.github.io/easy-rl/#/chapter2/chapter2_questions&amp;keywords">https://datawhalechina.github.io/easy-rl/#/chapter2/chapter2_questions&amp;keywords</a></p>
<p>第三章习题链接：<a href="https://datawhalechina.github.io/easy-rl/#/chapter3/chapter3_questions&amp;keywords">https://datawhalechina.github.io/easy-rl/#/chapter3/chapter3_questions&amp;keywords</a></p>
</blockquote>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>云原生初探</title>
    <url>/post/f3e182a8.html</url>
    <content><![CDATA[

	<div class="row">
    <embed src="/img/mydocument.pdf" width="100%" height="550" type="application/pdf">
	</div>



]]></content>
      <tags>
        <tag>云原生</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习算法备忘单</title>
    <url>/post/38dd791a.html</url>
    <content><![CDATA[<p>机器学习（ML）是人工智能（AI）和计算机科学的一个子领域，主要是利用数据和算法来模仿人的学习方式，逐步提高其准确性。使用这个树状图作为指南，以确定使用哪种ML算法来解决你的AI问题。</p>
<div align=center>
    <img src=https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/640.png width=90%></img>
<div style="display: inline-block; color: #999; padding: 2px;">图片来源：LatinX 在 AI™ 中的机器学习算法备忘单</div><br><br>
</div>
<p>如果你想知道在不同的应用程序中使用哪些机器学习算法，或者你是一个开发者，同时为你试图解决的问题寻找一种方法，请继续阅读下文，并以这些步骤作为指导。</p>
<p><ins>无监督学习</ins>使用未标记的信息数据，这样机器应该在没有指导的情况下根据模式、相似性和差异来工作。</p>
<p>另一方面，<ins>有监督学习</ins>有一个 “老师” 存在，他负责通过标记数据来训练机器工作。接下来，机器会收到一些示例，使其能够产生正确的结果。</p>
<p>但是对于这些类型的学习，有一种混合的方法，这种<ins>半监督学习</ins>适用于有标签和无标签的数据。这种方法使用一个极小的标记数据集来训练和标记其余数据，并进行相应的预测，最后给出问题的解决方案。</p>
<p>首先，你需要知道你所处理的维数，它意味着你的问题中输入的数量（也被称为特征）。如果你正在处理一个大数据集或许多特征，你可以选择降维算法。</p>
<h1>无监督学习：降维</h1>
<p>数据集合中的大量维度可能会对机器学习算法的性能产生重大影响。“维度诅咒” 是一个用来描述大维度可能造成的麻烦的术语，例如，聚类中的 “距离聚集（Distance Concentration）” 问题，即随着数据维度的增加，不同的数据点会有相同的值。</p>
<p>最小化训练数据中输入变量数量的技术被称为 “降维”。</p>
<p>现在你需要熟悉<ins>特征提取</ins>和<ins>特征选择</ins>的概念，以便继续学习。</p>
<p>将原始数据转化为可以处理的数字特征，同时保留原始数据集的信息，这一过程被称为<ins>特征提取</ins>。它比直接将机器学习应用于原始数据产生更好的结果。</p>
<p>它用于三种已知的降维算法，包括主成分分析、奇异值分解和线性判别分析，但你需要清楚地知道你想用哪种工具来寻找模式或从数据中推断出新的信息。</p>
<p>如果你不希望合并数据中的变量，而是想通过只保留重要的特征来去除不需要的特征，那么你可以使用主成分分析算法。</p>
<h2 id="PCA（主成分分析）">PCA（主成分分析）</h2>
<p>主成分分析是一种降低数据集维数的数学算法，在保留大部分信息的同时简化变量的数量。这种以准确性换取简单性的方法被广泛用于在大型数据集中寻找模式。</p>
<div align=center>
    <img src=https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/机器学习算法备忘单_1.png width=70%></img>
<div style="display: inline-block; color: #999; padding: 2px;">图片来源：https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/</div><br><br>
</div>
<p>在线性连接方面，它在有大量数据存在的情况下有着广泛的应用，如媒体编辑、统计质量控制、投资组合分析，以及人脸识别、图像压缩等许多应用。</p>
<p>另外，如果你想要一个通过组合你正在使用的数据的变量来工作的算法，简单的PCA可能不是你使用的最佳工具。接下来，你可以有一个概率模型或一个非概率模型。概率数据是涉及到随机选择的数据，是大多数科学家的首选，可以得到更准确的结果。而非概率数据不涉及这种随机性。</p>
<p>如果你正在处理非概率数据，你应该使用奇异值分解算法。</p>
<h2 id="SVD（奇异值分解）">SVD（奇异值分解）</h2>
<p>在机器学习领域，SVD允许数据被转化为一个可以轻松区分类别的空间。这种算法将一个矩阵分解为三个不同的矩阵。例如，在图像处理中，使用数量减少的矢量来重建与原始图像非常接近的图片。</p>
<div align=center>
    <img src=https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/机器学习算法备忘单_2.png width=100%></img>
<div style="display: inline-block; color: #999; padding: 2px;">使用给定数量的组件压缩图像</div><br><br>
</div>
<p>与PCA算法相比，两者都可以对数据进行降维处理。但PCA跳过了不太重要的成分，而SVD只是把它们变成特殊的数据，表示为三个不同的矩阵，更容易操作和分析。</p>
<p>当涉及到概率方法时，对于更抽象的问题，最好使用线性判别分析算法。</p>
<h2 id="LDA（线性判别分析）">LDA（线性判别分析）</h2>
<p>线性判别分析（LDA）是一种分类方法，在这种方法中，先前已经确定了两个或更多的组，根据其特征将新的观察结果归为其中一个。</p>
<p>它不同于PCA，因为LDA发现了一个优化组可分离性的特征子空间，而PCA忽略了类标签，专注于捕捉数据集的最高方差方向。</p>
<p>该算法使用贝叶斯定理，这是一个概率定理，用于根据一个事件与另一个事件的关系来确定其发生的可能性。</p>
<p>它经常被用于人脸识别、客户识别和医学领域，以识别病人的疾病状况。</p>
<div align=center>
    <img src=https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/机器学习算法备忘单_3.png width=900%></img>
</div>
从UMIST数据库中随机选取的五个主体（类）的170张人脸图像在（a）基于PCA的子空间，（b）基于D-LDA的子空间，以及（c）基于DF-LDA的子空间的分布。
<p>资料来源：Face recognition using LDA-based algorithms</p>
<p><a href="https://www.researchgate.net/publication/5613964">https://www.researchgate.net/publication/5613964</a></p>
<p>下一步是选择你是否希望你的算法有响应，这意味着你要开发一个基于标记数据的预测模型来教导你的机器。如果你愿意使用非标签数据，你可以使用聚类技术，这样你的机器就可以在没有指导的情况下工作，搜索相似性。</p>
<p>另一方面，选择相关特征（变量、预测因子）的子集用于模型创建的过程被称为<ins>特征选择</ins>。它有助于简化模型，使研究人员和用户更容易理解它们，以及减少训练周期和避免维度诅咒。</p>
<p>它包括聚类法、回归法和分类法。</p>
<h1>监督学习：聚类</h1>
<p>聚类是一种分离具有相似特征的群体并将其分配到群组的技术。</p>
<p>如果你正在寻找一种分层的算法：</p>
<h2 id="Hierarchical-Clustering（层次聚类）">Hierarchical Clustering（层次聚类）</h2>
<p>这种类型的聚类是机器学习中最流行的技术之一。层次聚类协助一个组织对数据进行分类，以确定相似性，以及不同的分组和特征，从而使其定价、商品、服务、营销信息和其他方面的业务有的放矢。它的层次结构应显示出类似于树状数据结构的数据，即所谓的树状图。有两种方法对数据进行分组：聚类和分化。</p>
<p><ins>聚合式聚类</ins>是一种 “自下而上” 的方法。换句话说，每个项目首先被认为是一个单元素集群（叶子）。在该方法的每个阶段，最具可比性的两个集群被连接成一个新的更大的集群（结点）。这种方法反复进行，直到所有的点都属于单个大簇（根）。</p>
<p><ins>分化聚类</ins>以一种 “自上而下” 的方式工作。它从根部开始，所有项目都分组在一个集群中，然后在每个迭代阶段将最多的项目分成两个。迭代程序直到所有的项目都在他们的组中。</p>
<p>如果你不寻找分层解决方案，则必须确定你的方法是否需要指定要使用的集群数量。如果你不需要定义，你可以利用基于密度的有噪声的应用程序空间聚类算法。</p>
<h2 id="DBSCAN（基于密度的有噪声的应用程序空间聚类法）">DBSCAN（基于密度的有噪声的应用程序空间聚类法）</h2>
<p>当涉及到任意形状的聚类或检测异常值时，最好使用基于密度的聚类方法。DBSCAN是一种检测那些任意形状的聚类和有噪声的聚类方法，它根据两个参数：eps和minPoints将彼此接近的点分组。</p>
<p>eps告诉我们两个点之间需要有多大的距离才能被视为一个集群。而minPoints是创建一个集群的最小点数。</p>
<p>我们在分析Netflix服务器的异常值时使用了这种算法。流媒体服务运行着数以千计的服务器，通常只有不到百分之一的服务器能够变得不健康，这会降低流媒体的性能。真正的问题是这个问题不容易被发现，为了解决这个问题，Netflix使用DBSCAN指定一个要监测的指标，然后收集数据，最后传递给算法来检测服务器的异常值。</p>
<div align=center>
    <img src=https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/机器学习算法备忘单_4.png width=90%></img>
<div style="display: inline-block; color: #999; padding: 2px;">资料来源：Tracking down the Villains: Outlier Detection at Netflix</div><br><br>
</div>
<p>日常使用可以是电子商务向客户推荐产品。对用户之前购买过的产品数据应用DBSCAN。</p>
<p>如果你需要指定聚类的数量，有三种现有的算法可供使用，包括K-Modes、K-Means和高斯混合模型。接下来，你需要知道是否要使用分类变量，这是一种离散变量，通过对观察值进行分组来捕捉定性的后果。如果你要使用它们，你可以选择K-Modes。</p>
<h2 id="K-Modes">K-Modes</h2>
<p>这种方法被用来对分类变量进行分组。我们确定这些类型的数据点之间的总不匹配度。我们的数据点之间的差异越少，它们就越相似。</p>
<p>K-Modes和K-Means之间的主要区别是：对于分类数据点，我们不能计算距离，因为它们不是数字值。</p>
<p>这种算法被用于文本挖掘应用、文档聚类、主题建模（每个聚类组代表一个特定的主题）、欺诈检测系统和市场营销。</p>
<p>对于数值型数据，你应该使用K-Means聚类。</p>
<h2 id="K-Means">K-Means</h2>
<p>数据被聚类为k个组，其方式是同一聚类中的数据点是相关的，而其他聚类中的数据点则相距较远。这种距离经常用欧几里得距离来衡量。换句话说，K-Means算法试图最小化聚类内的距离，最大化不同聚类之间的距离。</p>
<p>搜索引擎、消费者细分、垃圾邮件检测系统、学术表现、缺陷诊断系统、无线通信和许多其他行业都使用K-Means聚类。</p>
<p>如果预期的结果是基于概率的，那么你应该使用高斯混合模型。</p>
<h2 id="GMM（高斯混合模型）">GMM（高斯混合模型）</h2>
<p>这种方法意味着存在许多高斯分布，每个高斯分布代表一个集群。该算法将确定每个数据点属于给定批次数据的每个分布的概率。</p>
<p>GMM与K-Means不同，因为在GMM中，我们不知道一个数据点是否属于一个指定的聚类，我们使用概率来表达这种不确定性。而K-Means方法对一个数据点的位置是确定的，并开始在整个数据集上迭代。</p>
<p>高斯混合模型经常被用于信号处理、语言识别、异常检测和音乐的流派分类。</p>
<p>在使用标记数据来训练机器的情况下，首先，你需要指定它是否要预测数字，这种数字预测将有助于算法解决问题。如果是这样的话，你可以选择回归算法。</p>
<h1>监督学习：回归</h1>
<p>回归是一种机器学习算法，其结果被预测为一个连续的数值。这种方法通常用于银行、投资和其他领域。</p>
<p>在这里，你需要对速度和准确性做出取舍。如果你正在寻找速度，你可以使用决策树算法或线性回归算法。</p>
<h2 id="决策树">决策树</h2>
<p>决策树是一个类似树形数据结构的流程图。在这里，数据根据一个给定的参数被连续分割。每个参数允许在一个树节点中，而整个树的结果位于叶子中。有两种类型的决策树。</p>
<p>决策树是一个类似树形数据结构的流程图。在这里，数据根据一个给定的参数被连续分割。每个参数允许在一个树节点中，而整个树的结果位于叶子中。有两种类型的决策树。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>分类树（是/否类型），这里的决策变量是分类的。</p>
</li>
<li class="lvl-2">
<p>回归树（连续数据类型），这里的决策或结果变量是连续的。</p>
</li>
</ul>
<p>当特征和输出变量之间存在复杂的相互作用时，决策树就会派上用场。当存在缺失的特征，类别和数字特征的混合，或特征大小的巨大差异时，与其他方法相比，它们的表现更好。</p>
<p>该算法用于提高促销活动的准确性、欺诈检测以及患者严重或可预防疾病的检测。</p>
<h2 id="线性回归">线性回归</h2>
<p>基于一个给定的自变量，这种方法预测因变量的值。因此，这种回归方法决定了输入（自变量）和输出（因变量）之间是否存在线性联系。这也是线性回归这一术语的由来。</p>
<p>线性回归非常适合于那些特征和输出变量具有线性关系的数据集。</p>
<p>它通常用于预测（这对小公司了解销售效果特别有用），了解广告支出和收入之间的联系，以及在医疗行业了解药物剂量和病人血压之间的相关性。</p>
<p>另外，如果你的算法需要准确性，你可以使用以下三种算法。神经网络、梯度提升树和随机森林。</p>
<h2 id="神经网络">神经网络</h2>
<p>需要一个神经网络来学习特征和目标之间复杂的非线性关系。它是一种模拟人脑中神经元工作的算法。有几种类型的神经网络，包括香草神经网络（只处理结构化数据），以及循环神经网络和卷积神经网络，它们都可以处理非结构化数据。</p>
<p>当你有大量的数据（和处理能力），并且准确性对你很重要时，你几乎肯定会利用神经网络。</p>
<p>这种算法有很多应用，例如释义检测、文本分类、语义解析和问答。</p>
<h2 id="Gradient-Boosting-Tree（梯度提升树）">Gradient Boosting Tree（梯度提升树）</h2>
<p>梯度提升树是一种将不同树的输出合并进行回归或分类的方法。这两种监督学习都结合了大量的决策树，以减少每棵树单独面对的过拟合的危险（一种统计建模错误，当一个函数与少量数据点过于紧密匹配时，就会出现这种情况，使得模型的预测能力下降）。这种算法采用了<em>Boosting</em>，它需要连续组合弱学习器(通常是只有一次分裂的决策树，称为决策树桩)，以便每棵新树都纠正前一棵树的错误。</p>
<p>当我们希望减少偏差误差时，也就是模型的预测与目标值之间的差异，我们通常采用梯度提升算法。</p>
<p>当数据的维度较少，基本的线性模型表现不佳，可解释性并不重要，而且没有严格的延迟限制时，梯度提升算法是最有利的。</p>
<p>它被用在很多研究中，比如基于大师级运动员动机的性别预测算法，使用梯度提升决策树，探索他们基于心理维度预测性别的能力，评估参加大师级运动的原因作为统计方法。</p>
<h2 id="随机森林">随机森林</h2>
<p>随机森林是一种解决回归和分类问题的方法。它利用了集成学习，这是一种通过结合几个分类器来解决复杂问题的技术。</p>
<p>它由许多决策树组成，其中每一个决策树的结果都会以平均或平均决策的方式得出最终结果。树的数量越多，结果的精确度就越高。</p>
<p>当我们有一个巨大的数据集并且可解释性不是一个关键问题时，随机森林是合适的，因为随着数据集的增大，它变得越来越难以把握。</p>
<p>这种算法被用于股票市场分析、医疗领域的病人诊断、预测贷款申请人的信用度，以及欺诈检测。</p>
<p>对于非数字预测算法，你可以选择分类方法而不是回归。</p>
<h1>监督学习：分类</h1>
<p>与回归方法一样，你选择的结果是偏向于速度还是准确性。</p>
<p>如果你在寻找准确性，你不仅可以选择核支持向量机，还可以使用之前提到的其他算法，如神经网络、梯度提升树和随机森林。现在，让我们来介绍一下这个新算法。</p>
<h2 id="Kernel-Support-Vector-Machine（核支持向量机）">Kernel Support Vector Machine（核支持向量机）</h2>
<p>在支持向量机模型中，通常使用核技术来连接线性和非线性。为了理解这一点，有必要知道SVM方法学习如何通过形成决策边界来分离不同的组。</p>
<p>但是，当我们在一个维度较高的数据集面前，而且成本昂贵时，建议使用这种核方法。它使我们能够在原始特征空间中工作，而不必在高维空间中计算数据的坐标。</p>
<p>它主要用于文本分类问题，因为大多数问题都可以被线性分离。</p>
<p>当需要速度的时候，我们需要看看我们要采用的技术是否是可解释的，这意味着它可以解释你的模型中从头到尾发生了什么。在这种情况下，我们可能会使用决策树算法或Logistic回归算法。</p>
<h2 id="Logistic-Regression（逻辑回归）">Logistic Regression（逻辑回归）</h2>
<p>当因变量是分类的时候，就会使用Logistic回归。通过概率估计，它有助于理解因变量和一个或多个自变量之间的联系。</p>
<p>有三种不同类型的Logistic回归。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>二元逻辑回归，响应只有两个可能的值。</p>
</li>
<li class="lvl-2">
<p>多项式Logistic回归，三个或更多的结果，没有顺序。</p>
</li>
<li class="lvl-2">
<p>有序逻辑回归，三个或更多的类别，有顺序。</p>
</li>
</ul>
<p>逻辑回归算法在酒店预订中被广泛使用，它（通过统计研究）向你展示了你在预订中可能想要的选项，如酒店房间、该地区的一些行程等等。</p>
<p>如果你只对问题的输入和输出感兴趣，你可以检查你所处理的数据是否太大。如果数量很大，你可以使用线性支持向量机。</p>
<h2 id="Linear-Support-Vector-Machine（线性支持向量机）">Linear Support Vector Machine（线性支持向量机）</h2>
<p>线性SVM用于线性可分离的数据。它在具有不同变量的数据（线性可分离数据）中工作，这些变量可以用一条简单的直线（线性SVM分类器）来分离。这条直线代表了用户的行为或通过既定问题的结果。</p>
<p>由于文本通常是线性可分离的，并且有很多特征，因此线性SVM是用于其分类的最佳选择。</p>
<p>在我们的下一个算法中，如果数据量大或者不大，你都可以使用它。</p>
<h2 id="Naive-Bayes（朴素贝叶斯）">Naïve Bayes（朴素贝叶斯）</h2>
<p>这种算法是基于贝叶斯定理的。它包括通过对象的概率进行预测。它被称为Naïve（朴素），是因为它假设一个特征的出现与其他特征的出现无关。</p>
<p>这种方法深受欢迎，因为它甚至可以超越最复杂的分类方法。此外，它构造简单，可迅速建立。</p>
<p>由于其易于使用和高效，它被用来做实时决策。与此同时，Gmail使用这种算法来知道一封邮件是否是垃圾邮件。</p>
<p>Gmail垃圾邮件检测选择一组词或 “标记” 来识别垃圾邮件（这种方法也用于文本分类，它通常被称为词袋）。接下来，他们使用这些tokens（令牌），将其与垃圾邮件和非垃圾邮件进行比较。最后，使用Naïve Bayes算法，他们计算出该邮件是否是垃圾邮件的概率。</p>
<h1>总结</h1>
<p>我们发现，机器学习是一种被广泛使用的技术，由于它经常发生，因此我们无法识别许多应用。在这篇文章中，我们不仅区分了机器学习的不同方法，还区分了如何根据我们正在处理的数据和我们想要解决的问题来使用它们。</p>
<p>要学习机器学习，你必须具备一些微积分、线性代数、统计学和编程技能的知识。你可以使用不同的编程语言来实现其中一种算法，从Python到C++，以及R语言。这取决于你做出最好的决定，并与你的机器一起开始学习。</p>
<blockquote>
<p>原文链接🔗：<a href="https://medium.com/accel-ai/machine-learning-algorithms-cheat-sheet-990104aaaabc">https://medium.com/accel-ai/machine-learning-algorithms-cheat-sheet-990104aaaabc</a></p>
</blockquote>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>大学本科经验分享</title>
    <url>/post/3610a686.html</url>
    <content><![CDATA[<h1>个人档案</h1>
<div align=center>
    <img src=https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/大学本科经验分享_1.png width=70%></img>
</div>
<h1>打卡笔记六 “不” 走</h1>
<h2 id="六边形战士-人生“不”设限">六边形战士,人生“不”设限</h2>
<p>​	大学以来，我除了做好专业学习和科研竞赛外，还<ins>积极参与到社会实践和学生工作中</ins>。身处大学这样一个充满机遇的平台，我们不能让自己仅仅局限在学习能力和科研能力的提高上。参与社会实践、学生工作能够<ins>认识到比自己优秀的同学，与优秀的同学为伍，自己也会变得更优秀。</ins></p>
<div align=center>
    <img src=https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/大学本科经验分享_2.png width=80%></img>
</div>
<h2 id="生活“不”abandon-英语学习很重要">生活“不”abandon,英语学习很重要</h2>
<p>​	在高校夏令营或预推免中，大部分双一流高校都要求通过六级，有些高校还会对六级成绩进行规定。如果没有过六级，很多高校的简历初筛都过不了。在保研面试中，也会有英文问答、英文自我介绍环节，这一部分的分数通常占比15%-40%。你的六级成绩越高分，在保研面试中就有越大的优势。</p>
<h2 id="勇者“不”退缩-不服就比比">勇者“不”退缩,不服就比比</h2>
<p>​	我一直都鼓励我的师弟师妹们++多参加比赛，因为比赛的过程中，我们会对专业知识有更深刻的理解，从而更快掌握专业知识。++对于工科生来说，可参加的比赛是相当多的，数学建模、数学竞赛、程序设计大赛、计算机设计大赛、互联网+、挑战杯等都值得参加。</p>
<p>​	在大一，我跟很多同学一样，会因为害怕当“分母”而不敢参赛。后来，我在老师和师兄师姐的鼓励下，大胆迈出第一步。刚开始参赛时，我基本拿不到奖项，但我依然不放弃，继续在参赛路上砥砺前行。不过比起结果，我<ins>更享受参赛过程中自己点滴的进步</ins>。因为参加了比较多的比赛，我学到了很多新知识，能力也有所提高，拥有了夯实的学科基础。</p>
<p>​	而在今年的互联网+大赛中，申报的两个赛道都取得了全国总决赛银奖的成绩，正所谓 “故不积跬步，无以至千里；不积小流，无以成江海。” 因此，我觉得参加创新创业比赛对我大学生涯的影响可以有以下几点：</p>
<p>​	<ins>（1）个人能力的提升。</ins> 如果仅仅是听课，创新创业思维是很难锻炼的，只有去真正的实践，在实践中遇到问题并解决问题，才能慢慢形成创新创业思维。而创新创业比赛便是最好的实践平台，在比赛中能够促进自己去自学新知识，开阔视野，将所学的专业知识运用于实践。如果作品足够优秀，还能够得到主办方一定的支持。也正是因为参加这些比赛，我的自学能力、动手实践能力、组织能力、沟通能力等等都得到一定的提高。</p>
<p>​	<ins>（2）认识到更多优秀的人。</ins> 创新创业比赛基本都是团体性的比赛，需要与其他同学合作，在这期间也能够认识到更多志同道合、优秀的同学，与优秀的同学为伍，自己也会变得更优秀。</p>
<p>​	<ins>（3）助力深造和就业。</ins> 通过参加创新创业比赛，在得到锻炼的同时，也能收获相应的奖项和荣誉，这些奖项能够证明自己的能力，在考研、保研或者是就业面试中都能够起到一些作用。</p>
<h2 id="学习步履“不”停-一“研”为定">学习步履“不”停,一“研”为定</h2>
<p>​	在本科阶段真正地参与一些科研项目，可以帮你<ins>认清实际的科研是怎样的，能够让你认识到自己是否适合读研</ins>。同时，科研项目也能够将所学的知识最大程度地串起来，帮助自己<ins>找到感兴趣的研究方向，从而进行更深入的研究学习</ins>。除此之外，科研项目也能够丰富自己的简历，让你在就业或考研、保研面试有更多的优势。</p>
<div align=center>
    <img src=https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/大学本科经验分享_3.png width=80%></img>
</div>
<h2 id="“不”拖延-打卡少年有妙招">“不”拖延,打卡少年有妙招</h2>
<p>​	学习和竞赛本身是不冲突的事情，学习专业知识能够在参赛时有更多理论支撑，参加竞赛能够让自己学以致用，加深对知识的理解与掌握。</p>
<p>​	关于时间管理，首先我是将时间分为三大部分，分别是寒暑假、每个学期的前半个学期、每个学期的后半个学期，寒暑假我主要参与科研项目、参加社会实践活动，每个学期的前半个学期主要是参与学生工作、参加比赛、做好学期规划，每个学期的后半个学期主要是学习专业知识、打牢专业基础。当然并不是每一个阶段只做这些事情，而是这些事情在相应阶段是主要的安排。我认为在每个阶段有侧重点去学习或工作，才能达到较高的效率。除此之外，每天晚上睡前我都会把第二天做的事情在手机备忘录上列出来，我不会做详细的计划（像哪个时间段做哪些事情），详细的计划会让我觉得很难执行，所以都是只列出要做的事情而不规定时间段，每完成一件事情就在备忘录左边打勾，这样也比较有成就感，能够让自己每天都过得很充实，并且很少有不知道做什么或者发呆的时候。</p>
<div align=center>
    <img src=https://mgt-1301264585.cos.ap-guangzhou.myqcloud.com/img/大学本科经验分享_4.png width=40%></img>
</div>
<h2 id="“不”忘初心-走向星辰大海">“不”忘初心,走向星辰大海</h2>
<p>​	保研是我大一就树立的目标，在过去的三年里，我一直在为这个目标而奋斗。虽然过程中遇到很多挫折，也曾让我怀疑过自己，但在身边朋友、老师和家人的帮助下，在不断尝试下，最终达成目标。</p>
<p>​	我建议在读大学的你们尽早确定方向，朝着自己的目标扬帆起航。当然，保研对绩点和科研竞赛有较高的要求。如果你选择了保研，就代表你将走上一条竞争激烈的道路。在走向成功的路上，不要因为小挫折而失去信心，要勇于抓住眼前的每一个机会。同时，也要善于总结与反思自己每个阶段所做的事情，并不断为自己树立小目标。</p>
<div align=center>
热血燃烧 青春闪耀</br>
未来在即 梦想可期</br>
愿大家都能实现自己的梦想！</br>
</div>]]></content>
      <tags>
        <tag>经验分享</tag>
      </tags>
  </entry>
</search>
